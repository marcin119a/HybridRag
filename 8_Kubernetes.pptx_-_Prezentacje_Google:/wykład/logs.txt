
==> Audyt <==
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  COMMAND   â”‚      ARGS       â”‚ PROFILE  â”‚    USER     â”‚ VERSION â”‚     START TIME      â”‚      END TIME       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ start      â”‚                 â”‚ minikube â”‚ student-030 â”‚ v1.37.0 â”‚ 18 Nov 25 16:00 CET â”‚ 18 Nov 25 16:01 CET â”‚
â”‚ docker-env â”‚                 â”‚ minikube â”‚ student-030 â”‚ v1.37.0 â”‚ 18 Nov 25 16:23 CET â”‚ 18 Nov 25 16:23 CET â”‚
â”‚ ip         â”‚                 â”‚ minikube â”‚ student-030 â”‚ v1.37.0 â”‚ 18 Nov 25 16:28 CET â”‚ 18 Nov 25 16:28 CET â”‚
â”‚ completion â”‚ bash            â”‚ minikube â”‚ student-030 â”‚ v1.37.0 â”‚ 18 Nov 25 16:29 CET â”‚ 18 Nov 25 16:29 CET â”‚
â”‚ service    â”‚ fastapi-service â”‚ minikube â”‚ student-030 â”‚ v1.37.0 â”‚ 18 Nov 25 16:29 CET â”‚                     â”‚
â”‚ ip         â”‚                 â”‚ minikube â”‚ student-030 â”‚ v1.37.0 â”‚ 18 Nov 25 16:31 CET â”‚ 18 Nov 25 16:31 CET â”‚
â”‚ service    â”‚ fastapi-service â”‚ minikube â”‚ student-030 â”‚ v1.37.0 â”‚ 18 Nov 25 16:32 CET â”‚                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


==> Ostatni start <==
Log file created at: 2025/11/18 16:00:33
Running on machine: vm-2bcfdb57
Binary: Built with gc go1.24.6 for linux/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I1118 16:00:33.786350   75355 out.go:360] Setting OutFile to fd 1 ...
I1118 16:00:33.786838   75355 out.go:413] isatty.IsTerminal(1) = true
I1118 16:00:33.786843   75355 out.go:374] Setting ErrFile to fd 2...
I1118 16:00:33.786848   75355 out.go:413] isatty.IsTerminal(2) = true
I1118 16:00:33.787364   75355 root.go:338] Updating PATH: /home/student/.minikube/bin
W1118 16:00:33.787718   75355 root.go:314] Error reading config file at /home/student/.minikube/config/config.json: open /home/student/.minikube/config/config.json: no such file or directory
I1118 16:00:33.788468   75355 out.go:368] Setting JSON to false
I1118 16:00:33.800935   75355 start.go:130] hostinfo: {"hostname":"vm-2bcfdb57","uptime":510630,"bootTime":1762967403,"procs":281,"os":"linux","platform":"debian","platformFamily":"debian","platformVersion":"13.1","kernelVersion":"6.12.48+deb13-amd64","kernelArch":"x86_64","virtualizationSystem":"kvm","virtualizationRole":"guest","hostId":"2bcfdb57-0dc4-423a-8cab-9778b0e1c413"}
I1118 16:00:33.801355   75355 start.go:140] virtualization: kvm guest
I1118 16:00:33.803119   75355 out.go:179] ðŸ˜„  minikube v1.37.0 na Debian 13.1 (kvm/amd64)
I1118 16:00:33.806295   75355 notify.go:220] Checking for updates...
W1118 16:00:33.806681   75355 preload.go:293] Failed to list preload files: open /home/student/.minikube/cache/preloaded-tarball: no such file or directory
I1118 16:00:33.806687   75355 driver.go:421] Setting default libvirt URI to qemu:///system
I1118 16:00:33.806714   75355 global.go:112] Querying for installed drivers using PATH=/home/student/.minikube/bin:/home/student/HybridRag/.venv/bin:/home/student/.vscode-server/data/User/globalStorage/github.copilot-chat/debugCommand:/home/student/.vscode-server/data/User/globalStorage/github.copilot-chat/copilotCli:/home/student/.vscode-server/cli/servers/Stable-ac4cbdf48759c7d8c3eb91ffe6bb04316e263c57/server/bin/remote-cli:/home/student/opt/poetry/bin:/home/student/opt/poetry/bin:/home/student/.cargo/bin:/home/student/.nvm/versions/node/v22.20.0/bin:/home/student/.sdkman/candidates/java/current/bin:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games:/home/student/.vscode-server/extensions/ms-python.debugpy-2025.16.0/bundled/scripts/noConfigScripts
I1118 16:00:33.806736   75355 global.go:133] ssh default: false priority: 4, state: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I1118 16:00:33.806877   75355 global.go:133] kvm2 default: true priority: 8, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "virsh": executable file not found in $PATH Reason: Fix:Install libvirt Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/kvm2/ Version:}
I1118 16:00:33.806973   75355 global.go:133] qemu2 default: true priority: 7, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "qemu-system-x86_64": executable file not found in $PATH Reason: Fix:Install qemu-system Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/qemu/ Version:}
I1118 16:00:33.807075   75355 global.go:133] virtualbox default: true priority: 6, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:unable to find VBoxManage in $PATH Reason: Fix:Install VirtualBox Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/virtualbox/ Version:}
I1118 16:00:33.807128   75355 global.go:133] vmware default: false priority: 5, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "vmrun": executable file not found in $PATH Reason: Fix:Install vmrun Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/vmware/ Version:}
I1118 16:00:33.842622   75355 docker.go:123] docker version: linux-29.0.0:Docker Engine - Community
I1118 16:00:33.842731   75355 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I1118 16:00:34.042796   75355 info.go:266] docker info: {ID:7c577e2e-910a-4e32-85a5-823a3e65cccf Containers:11 ContainersRunning:0 ContainersPaused:0 ContainersStopped:11 Images:11 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:23 OomKillDisable:false NGoroutines:44 SystemTime:2025-11-18 16:00:34.034145637 +0100 CET LoggingDriver:json-file CgroupDriver:systemd NEventsListener:0 KernelVersion:6.12.48+deb13-amd64 OperatingSystem:Debian GNU/Linux 13 (trixie) OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8 ::1/128] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:16775327744 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:vm-2bcfdb57 Labels:[] ExperimentalBuild:false ServerVersion:29.0.0 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:fcd43222d6b07379a4be9786bda52438f0dd16a1 Expected:} RuncCommit:{ID:v1.3.3-0-gd842d771 Expected:} InitCommit:{ID:de40ad0 Expected:} SecurityOptions:[name=apparmor name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/libexec/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.29.1] map[Name:compose Path:/usr/libexec/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.40.3]] Warnings:<nil>}}
I1118 16:00:34.042889   75355 docker.go:318] overlay module found
I1118 16:00:34.042910   75355 global.go:133] docker default: true priority: 9, state: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I1118 16:00:34.043328   75355 global.go:133] none default: false priority: 4, state: {Installed:false Healthy:false Running:true NeedsImprovement:false Error:exec: "iptables": executable file not found in $PATH Reason: Fix:iptables must be installed Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/none/ Version:}
I1118 16:00:34.043590   75355 global.go:133] podman default: true priority: 7, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "podman": executable file not found in $PATH Reason: Fix:Install Podman Doc:https://minikube.sigs.k8s.io/docs/drivers/podman/ Version:}
I1118 16:00:34.043616   75355 driver.go:343] not recommending "ssh" due to default: false
I1118 16:00:34.043637   75355 driver.go:378] Picked: docker
I1118 16:00:34.043649   75355 driver.go:379] Alternatives: [ssh]
I1118 16:00:34.043653   75355 driver.go:380] Rejects: [kvm2 qemu2 virtualbox vmware none podman]
I1118 16:00:34.045759   75355 out.go:179] âœ¨  Automatycznie wybrano sterownik docker
I1118 16:00:34.047436   75355 start.go:304] selected driver: docker
I1118 16:00:34.047441   75355 start.go:918] validating driver "docker" against <nil>
I1118 16:00:34.047448   75355 start.go:929] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I1118 16:00:34.047556   75355 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I1118 16:00:34.094027   75355 info.go:266] docker info: {ID:7c577e2e-910a-4e32-85a5-823a3e65cccf Containers:11 ContainersRunning:0 ContainersPaused:0 ContainersStopped:11 Images:11 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:23 OomKillDisable:false NGoroutines:44 SystemTime:2025-11-18 16:00:34.085166652 +0100 CET LoggingDriver:json-file CgroupDriver:systemd NEventsListener:0 KernelVersion:6.12.48+deb13-amd64 OperatingSystem:Debian GNU/Linux 13 (trixie) OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8 ::1/128] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:16775327744 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:vm-2bcfdb57 Labels:[] ExperimentalBuild:false ServerVersion:29.0.0 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:fcd43222d6b07379a4be9786bda52438f0dd16a1 Expected:} RuncCommit:{ID:v1.3.3-0-gd842d771 Expected:} InitCommit:{ID:de40ad0 Expected:} SecurityOptions:[name=apparmor name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/libexec/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.29.1] map[Name:compose Path:/usr/libexec/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.40.3]] Warnings:<nil>}}
I1118 16:00:34.094261   75355 start_flags.go:327] no existing cluster config was found, will generate one from the flags 
I1118 16:00:34.094596   75355 start_flags.go:410] Using suggested 3900MB memory alloc based on sys=15998MB, container=15998MB
I1118 16:00:34.095451   75355 start_flags.go:974] Wait components to verify : map[apiserver:true system_pods:true]
I1118 16:00:34.097211   75355 out.go:179] ðŸ“Œ  Using Docker driver with root privileges
I1118 16:00:34.098704   75355 cni.go:84] Creating CNI manager for ""
I1118 16:00:34.098799   75355 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1118 16:00:34.098816   75355 start_flags.go:336] Found "bridge CNI" CNI - setting NetworkPlugin=cni
I1118 16:00:34.098968   75355 start.go:348] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 Memory:3900 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s MountString: Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false DisableCoreDNSLog:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I1118 16:00:34.100739   75355 out.go:179] ðŸ‘  Starting "minikube" primary control-plane node in "minikube" cluster
I1118 16:00:34.102450   75355 cache.go:123] Beginning downloading kic base image for docker with docker
I1118 16:00:34.104452   75355 out.go:179] ðŸšœ  Pulling base image v0.0.48 ...
I1118 16:00:34.107141   75355 preload.go:131] Checking if preload exists for k8s version v1.34.0 and runtime docker
I1118 16:00:34.107466   75355 image.go:81] Checking for gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 in local docker daemon
I1118 16:00:34.124984   75355 cache.go:152] Downloading gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 to local cache
I1118 16:00:34.125483   75355 image.go:65] Checking for gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 in local cache directory
I1118 16:00:34.125573   75355 image.go:150] Writing gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 to local cache
I1118 16:00:34.564199   75355 preload.go:118] Found remote preload: https://storage.googleapis.com/minikube-preloaded-volume-tarballs/v18/v1.34.0/preloaded-images-k8s-v18-v1.34.0-docker-overlay2-amd64.tar.lz4
I1118 16:00:34.564223   75355 cache.go:58] Caching tarball of preloaded images
I1118 16:00:34.564348   75355 preload.go:131] Checking if preload exists for k8s version v1.34.0 and runtime docker
I1118 16:00:34.566472   75355 out.go:179] ðŸ’¾  Downloading Kubernetes v1.34.0 preload ...
I1118 16:00:34.569120   75355 preload.go:236] getting checksum for preloaded-images-k8s-v18-v1.34.0-docker-overlay2-amd64.tar.lz4 ...
I1118 16:00:34.904698   75355 download.go:108] Downloading: https://storage.googleapis.com/minikube-preloaded-volume-tarballs/v18/v1.34.0/preloaded-images-k8s-v18-v1.34.0-docker-overlay2-amd64.tar.lz4?checksum=md5:994a4de1464928e89c992dfd0a962e35 -> /home/student/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.34.0-docker-overlay2-amd64.tar.lz4
I1118 16:00:42.838827   75355 cache.go:155] successfully saved gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 as a tarball
I1118 16:00:42.838862   75355 cache.go:165] Loading gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 from local cache
I1118 16:00:46.136591   75355 preload.go:247] saving checksum for preloaded-images-k8s-v18-v1.34.0-docker-overlay2-amd64.tar.lz4 ...
I1118 16:00:46.136673   75355 preload.go:254] verifying checksum of /home/student/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.34.0-docker-overlay2-amd64.tar.lz4 ...
I1118 16:00:46.695483   75355 cache.go:61] Finished verifying existence of preloaded tar for v1.34.0 on docker
I1118 16:00:46.695751   75355 profile.go:143] Saving config to /home/student/.minikube/profiles/minikube/config.json ...
I1118 16:00:46.695773   75355 lock.go:35] WriteFile acquiring /home/student/.minikube/profiles/minikube/config.json: {Name:mk170776d8f39c567a96ebc63fd48b3d3991d50a Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1118 16:00:53.628700   75355 cache.go:167] successfully loaded and using gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 from cached tarball
I1118 16:00:53.628732   75355 cache.go:232] Successfully downloaded all kic artifacts
I1118 16:00:53.628764   75355 start.go:360] acquireMachinesLock for minikube: {Name:mk2dc08f1ea4bbb4d459bf2b3fb52870223d421e Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I1118 16:00:53.628856   75355 start.go:364] duration metric: took 73.875Âµs to acquireMachinesLock for "minikube"
I1118 16:00:53.628878   75355 start.go:93] Provisioning new machine with config: &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 Memory:3900 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s MountString: Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false DisableCoreDNSLog:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} &{Name: IP: Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I1118 16:00:53.628938   75355 start.go:125] createHost starting for "" (driver="docker")
I1118 16:00:53.631625   75355 out.go:252] ðŸ”¥  Creating docker container (CPUs=2, Memory=3900MB) ...
I1118 16:00:53.632613   75355 start.go:159] libmachine.API.Create for "minikube" (driver="docker")
I1118 16:00:53.632650   75355 client.go:168] LocalClient.Create starting
I1118 16:00:53.632745   75355 main.go:141] libmachine: Creating CA: /home/student/.minikube/certs/ca.pem
I1118 16:00:54.006554   75355 main.go:141] libmachine: Creating client certificate: /home/student/.minikube/certs/cert.pem
I1118 16:00:54.506943   75355 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
W1118 16:00:54.530297   75355 cli_runner.go:211] docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}" returned with exit code 1
I1118 16:00:54.530377   75355 network_create.go:284] running [docker network inspect minikube] to gather additional debugging logs...
I1118 16:00:54.530390   75355 cli_runner.go:164] Run: docker network inspect minikube
W1118 16:00:54.548027   75355 cli_runner.go:211] docker network inspect minikube returned with exit code 1
I1118 16:00:54.548045   75355 network_create.go:287] error running [docker network inspect minikube]: docker network inspect minikube: exit status 1
stdout:
[]

stderr:
Error response from daemon: network minikube not found
I1118 16:00:54.548055   75355 network_create.go:289] output of [docker network inspect minikube]: -- stdout --
[]

-- /stdout --
** stderr ** 
Error response from daemon: network minikube not found

** /stderr **
I1118 16:00:54.548157   75355 cli_runner.go:164] Run: docker network inspect bridge --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I1118 16:00:54.564239   75355 network.go:206] using free private subnet 192.168.49.0/24: &{IP:192.168.49.0 Netmask:255.255.255.0 Prefix:24 CIDR:192.168.49.0/24 Gateway:192.168.49.1 ClientMin:192.168.49.2 ClientMax:192.168.49.254 Broadcast:192.168.49.255 IsPrivate:true Interface:{IfaceName: IfaceIPv4: IfaceMTU:0 IfaceMAC:} reservation:0xc00174aa00}
I1118 16:00:54.564285   75355 network_create.go:124] attempt to create docker network minikube 192.168.49.0/24 with gateway 192.168.49.1 and MTU of 1500 ...
I1118 16:00:54.564469   75355 cli_runner.go:164] Run: docker network create --driver=bridge --subnet=192.168.49.0/24 --gateway=192.168.49.1 -o --ip-masq -o --icc -o com.docker.network.driver.mtu=1500 --label=created_by.minikube.sigs.k8s.io=true --label=name.minikube.sigs.k8s.io=minikube minikube
I1118 16:00:54.614570   75355 network_create.go:108] docker network minikube 192.168.49.0/24 created
I1118 16:00:54.614589   75355 kic.go:121] calculated static IP "192.168.49.2" for the "minikube" container
I1118 16:00:54.614700   75355 cli_runner.go:164] Run: docker ps -a --format {{.Names}}
I1118 16:00:54.630677   75355 cli_runner.go:164] Run: docker volume create minikube --label name.minikube.sigs.k8s.io=minikube --label created_by.minikube.sigs.k8s.io=true
I1118 16:00:54.654000   75355 oci.go:103] Successfully created a docker volume minikube
I1118 16:00:54.654089   75355 cli_runner.go:164] Run: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 -d /var/lib
I1118 16:00:55.487182   75355 oci.go:107] Successfully prepared a docker volume minikube
I1118 16:00:55.487216   75355 preload.go:131] Checking if preload exists for k8s version v1.34.0 and runtime docker
I1118 16:00:55.487234   75355 kic.go:194] Starting extracting preloaded images to volume ...
I1118 16:00:55.487318   75355 cli_runner.go:164] Run: docker run --rm --entrypoint /usr/bin/tar -v /home/student/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.34.0-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 -I lz4 -xf /preloaded.tar -C /extractDir
I1118 16:00:57.601251   75355 cli_runner.go:217] Completed: docker run --rm --entrypoint /usr/bin/tar -v /home/student/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.34.0-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 -I lz4 -xf /preloaded.tar -C /extractDir: (2.113896753s)
I1118 16:00:57.601271   75355 kic.go:203] duration metric: took 2.114036438s to extract preloaded images to volume ...
W1118 16:00:57.601370   75355 cgroups_linux.go:77] Your kernel does not support swap limit capabilities or the cgroup is not mounted.
W1118 16:00:57.601400   75355 oci.go:252] Your kernel does not support CPU cfs period/quota or the cgroup is not mounted.
I1118 16:00:57.601462   75355 cli_runner.go:164] Run: docker info --format "'{{json .SecurityOptions}}'"
I1118 16:00:57.645635   75355 cli_runner.go:164] Run: docker run -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname minikube --name minikube --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=minikube --network minikube --ip 192.168.49.2 --volume minikube:/var --security-opt apparmor=unconfined --memory=3900mb -e container=docker --expose 8443 --publish=127.0.0.1::8443 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1
I1118 16:00:57.995622   75355 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Running}}
I1118 16:00:58.012266   75355 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1118 16:00:58.028401   75355 cli_runner.go:164] Run: docker exec minikube stat /var/lib/dpkg/alternatives/iptables
I1118 16:00:58.072199   75355 oci.go:144] the created container "minikube" has a running status.
I1118 16:00:58.072216   75355 kic.go:225] Creating ssh key for kic: /home/student/.minikube/machines/minikube/id_rsa...
I1118 16:00:58.120683   75355 kic_runner.go:191] docker (temp): /home/student/.minikube/machines/minikube/id_rsa.pub --> /home/docker/.ssh/authorized_keys (381 bytes)
I1118 16:00:58.146564   75355 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1118 16:00:58.162358   75355 kic_runner.go:93] Run: chown docker:docker /home/docker/.ssh/authorized_keys
I1118 16:00:58.162367   75355 kic_runner.go:114] Args: [docker exec --privileged minikube chown docker:docker /home/docker/.ssh/authorized_keys]
I1118 16:00:58.221933   75355 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1118 16:00:58.238526   75355 machine.go:93] provisionDockerMachine start ...
I1118 16:00:58.238630   75355 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1118 16:00:58.257513   75355 main.go:141] libmachine: Using SSH client type: native
I1118 16:00:58.257732   75355 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x840140] 0x842e40 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I1118 16:00:58.257737   75355 main.go:141] libmachine: About to run SSH command:
hostname
I1118 16:00:58.258262   75355 main.go:141] libmachine: Error dialing TCP: ssh: handshake failed: read tcp 127.0.0.1:40766->127.0.0.1:32768: read: connection reset by peer
I1118 16:01:01.393395   75355 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I1118 16:01:01.393411   75355 ubuntu.go:182] provisioning hostname "minikube"
I1118 16:01:01.393488   75355 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1118 16:01:01.410430   75355 main.go:141] libmachine: Using SSH client type: native
I1118 16:01:01.410612   75355 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x840140] 0x842e40 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I1118 16:01:01.410618   75355 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I1118 16:01:01.554278   75355 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I1118 16:01:01.554367   75355 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1118 16:01:01.569905   75355 main.go:141] libmachine: Using SSH client type: native
I1118 16:01:01.570098   75355 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x840140] 0x842e40 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I1118 16:01:01.570107   75355 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I1118 16:01:01.701571   75355 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I1118 16:01:01.701585   75355 ubuntu.go:188] set auth options {CertDir:/home/student/.minikube CaCertPath:/home/student/.minikube/certs/ca.pem CaPrivateKeyPath:/home/student/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/home/student/.minikube/machines/server.pem ServerKeyPath:/home/student/.minikube/machines/server-key.pem ClientKeyPath:/home/student/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/home/student/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/home/student/.minikube}
I1118 16:01:01.701603   75355 ubuntu.go:190] setting up certificates
I1118 16:01:01.701611   75355 provision.go:84] configureAuth start
I1118 16:01:01.701696   75355 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1118 16:01:01.718421   75355 provision.go:143] copyHostCerts
I1118 16:01:01.718482   75355 exec_runner.go:151] cp: /home/student/.minikube/certs/ca.pem --> /home/student/.minikube/ca.pem (1090 bytes)
I1118 16:01:01.718592   75355 exec_runner.go:151] cp: /home/student/.minikube/certs/cert.pem --> /home/student/.minikube/cert.pem (1135 bytes)
I1118 16:01:01.718664   75355 exec_runner.go:151] cp: /home/student/.minikube/certs/key.pem --> /home/student/.minikube/key.pem (1675 bytes)
I1118 16:01:01.718717   75355 provision.go:117] generating server cert: /home/student/.minikube/machines/server.pem ca-key=/home/student/.minikube/certs/ca.pem private-key=/home/student/.minikube/certs/ca-key.pem org=student-030.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I1118 16:01:02.056346   75355 provision.go:177] copyRemoteCerts
I1118 16:01:02.056443   75355 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I1118 16:01:02.056502   75355 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1118 16:01:02.072965   75355 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/student/.minikube/machines/minikube/id_rsa Username:docker}
I1118 16:01:02.170737   75355 ssh_runner.go:362] scp /home/student/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1090 bytes)
I1118 16:01:02.203032   75355 ssh_runner.go:362] scp /home/student/.minikube/machines/server.pem --> /etc/docker/server.pem (1192 bytes)
I1118 16:01:02.226409   75355 ssh_runner.go:362] scp /home/student/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I1118 16:01:02.249510   75355 provision.go:87] duration metric: took 547.889317ms to configureAuth
I1118 16:01:02.249523   75355 ubuntu.go:206] setting minikube options for container-runtime
I1118 16:01:02.249656   75355 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.34.0
I1118 16:01:02.249743   75355 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1118 16:01:02.266924   75355 main.go:141] libmachine: Using SSH client type: native
I1118 16:01:02.267102   75355 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x840140] 0x842e40 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I1118 16:01:02.267107   75355 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I1118 16:01:02.401919   75355 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I1118 16:01:02.401930   75355 ubuntu.go:71] root file system type: overlay
I1118 16:01:02.402021   75355 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I1118 16:01:02.402100   75355 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1118 16:01:02.418319   75355 main.go:141] libmachine: Using SSH client type: native
I1118 16:01:02.418537   75355 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x840140] 0x842e40 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I1118 16:01:02.418597   75355 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network-online.target nss-lookup.target docker.socket firewalld.service containerd.service time-set.target
Wants=network-online.target containerd.service
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=always



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 \
	-H fd:// --containerd=/run/containerd/containerd.sock \
	-H unix:///var/run/docker.sock \
	--default-ulimit=nofile=1048576:1048576 \
	--tlsverify \
	--tlscacert /etc/docker/ca.pem \
	--tlscert /etc/docker/server.pem \
	--tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process
OOMScoreAdjust=-500

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I1118 16:01:02.585835   75355 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network-online.target nss-lookup.target docker.socket firewalld.service containerd.service time-set.target
Wants=network-online.target containerd.service
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=always



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 	-H fd:// --containerd=/run/containerd/containerd.sock 	-H unix:///var/run/docker.sock 	--default-ulimit=nofile=1048576:1048576 	--tlsverify 	--tlscacert /etc/docker/ca.pem 	--tlscert /etc/docker/server.pem 	--tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process
OOMScoreAdjust=-500

[Install]
WantedBy=multi-user.target

I1118 16:01:02.585931   75355 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1118 16:01:02.603297   75355 main.go:141] libmachine: Using SSH client type: native
I1118 16:01:02.603483   75355 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x840140] 0x842e40 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I1118 16:01:02.603494   75355 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I1118 16:01:04.413626   75355 main.go:141] libmachine: SSH cmd err, output: <nil>: --- /lib/systemd/system/docker.service	2025-09-03 20:55:49.000000000 +0000
+++ /lib/systemd/system/docker.service.new	2025-11-18 15:01:02.578976717 +0000
@@ -9,23 +9,34 @@
 
 [Service]
 Type=notify
-# the default is not to use systemd for cgroups because the delegate issues still
-# exists and systemd currently does not support the cgroup feature set required
-# for containers run by docker
-ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock
-ExecReload=/bin/kill -s HUP $MAINPID
-TimeoutStartSec=0
-RestartSec=2
 Restart=always
 
+
+
+# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
+# The base configuration already specifies an 'ExecStart=...' command. The first directive
+# here is to clear out that command inherited from the base configuration. Without this,
+# the command from the base configuration and the command specified here are treated as
+# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
+# will catch this invalid input and refuse to start the service with an error like:
+#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.
+
+# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
+# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
+ExecStart=
+ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 	-H fd:// --containerd=/run/containerd/containerd.sock 	-H unix:///var/run/docker.sock 	--default-ulimit=nofile=1048576:1048576 	--tlsverify 	--tlscacert /etc/docker/ca.pem 	--tlscert /etc/docker/server.pem 	--tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
+ExecReload=/bin/kill -s HUP $MAINPID
+
 # Having non-zero Limit*s causes performance problems due to accounting overhead
 # in the kernel. We recommend using cgroups to do container-local accounting.
+LimitNOFILE=infinity
 LimitNPROC=infinity
 LimitCORE=infinity
 
-# Comment TasksMax if your systemd version does not support it.
-# Only systemd 226 and above support this option.
+# Uncomment TasksMax if your systemd version supports it.
+# Only systemd 226 and above support this version.
 TasksMax=infinity
+TimeoutStartSec=0
 
 # set delegate yes so that systemd does not reset the cgroups of docker containers
 Delegate=yes
Synchronizing state of docker.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable docker

I1118 16:01:04.413649   75355 machine.go:96] duration metric: took 6.17511267s to provisionDockerMachine
I1118 16:01:04.413659   75355 client.go:171] duration metric: took 10.781005113s to LocalClient.Create
I1118 16:01:04.413678   75355 start.go:167] duration metric: took 10.781069753s to libmachine.API.Create "minikube"
I1118 16:01:04.413683   75355 start.go:293] postStartSetup for "minikube" (driver="docker")
I1118 16:01:04.413691   75355 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I1118 16:01:04.413768   75355 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I1118 16:01:04.413824   75355 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1118 16:01:04.430808   75355 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/student/.minikube/machines/minikube/id_rsa Username:docker}
I1118 16:01:04.536617   75355 ssh_runner.go:195] Run: cat /etc/os-release
I1118 16:01:04.539675   75355 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I1118 16:01:04.539690   75355 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I1118 16:01:04.539695   75355 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I1118 16:01:04.539700   75355 info.go:137] Remote host: Ubuntu 22.04.5 LTS
I1118 16:01:04.539707   75355 filesync.go:126] Scanning /home/student/.minikube/addons for local assets ...
I1118 16:01:04.539760   75355 filesync.go:126] Scanning /home/student/.minikube/files for local assets ...
I1118 16:01:04.539783   75355 start.go:296] duration metric: took 126.096628ms for postStartSetup
I1118 16:01:04.540048   75355 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1118 16:01:04.557205   75355 profile.go:143] Saving config to /home/student/.minikube/profiles/minikube/config.json ...
I1118 16:01:04.557472   75355 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I1118 16:01:04.557541   75355 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1118 16:01:04.573662   75355 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/student/.minikube/machines/minikube/id_rsa Username:docker}
I1118 16:01:04.669986   75355 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I1118 16:01:04.673921   75355 start.go:128] duration metric: took 11.044970729s to createHost
I1118 16:01:04.673933   75355 start.go:83] releasing machines lock for "minikube", held for 11.045071931s
I1118 16:01:04.674005   75355 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1118 16:01:04.691105   75355 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I1118 16:01:04.691104   75355 ssh_runner.go:195] Run: cat /version.json
I1118 16:01:04.691169   75355 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1118 16:01:04.691178   75355 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1118 16:01:04.707120   75355 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/student/.minikube/machines/minikube/id_rsa Username:docker}
I1118 16:01:04.707995   75355 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/student/.minikube/machines/minikube/id_rsa Username:docker}
I1118 16:01:04.796995   75355 ssh_runner.go:195] Run: systemctl --version
I1118 16:01:04.976420   75355 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I1118 16:01:04.980917   75355 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I1118 16:01:05.027090   75355 cni.go:230] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I1118 16:01:05.027165   75355 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%p, " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I1118 16:01:05.054573   75355 cni.go:262] disabled [/etc/cni/net.d/87-podman-bridge.conflist, /etc/cni/net.d/100-crio-bridge.conf] bridge cni config(s)
I1118 16:01:05.054590   75355 start.go:495] detecting cgroup driver to use...
I1118 16:01:05.054615   75355 detect.go:190] detected "systemd" cgroup driver on host os
I1118 16:01:05.055002   75355 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I1118 16:01:05.070699   75355 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.10.1"|' /etc/containerd/config.toml"
I1118 16:01:05.082102   75355 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I1118 16:01:05.091343   75355 containerd.go:146] configuring containerd to use "systemd" as cgroup driver...
I1118 16:01:05.091405   75355 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = true|g' /etc/containerd/config.toml"
I1118 16:01:05.100619   75355 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I1118 16:01:05.109898   75355 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I1118 16:01:05.119051   75355 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I1118 16:01:05.128129   75355 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I1118 16:01:05.137102   75355 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I1118 16:01:05.146316   75355 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I1118 16:01:05.155531   75355 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I1118 16:01:05.164923   75355 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I1118 16:01:05.172810   75355 crio.go:166] couldn't verify netfilter by "sudo sysctl net.bridge.bridge-nf-call-iptables" which might be okay. error: sudo sysctl net.bridge.bridge-nf-call-iptables: Process exited with status 255
stdout:

stderr:
sysctl: cannot stat /proc/sys/net/bridge/bridge-nf-call-iptables: No such file or directory
I1118 16:01:05.172862   75355 ssh_runner.go:195] Run: sudo modprobe br_netfilter
I1118 16:01:05.190433   75355 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I1118 16:01:05.199402   75355 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1118 16:01:05.289402   75355 ssh_runner.go:195] Run: sudo systemctl restart containerd
I1118 16:01:05.363322   75355 start.go:495] detecting cgroup driver to use...
I1118 16:01:05.363353   75355 detect.go:190] detected "systemd" cgroup driver on host os
I1118 16:01:05.363425   75355 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I1118 16:01:05.374585   75355 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service containerd
I1118 16:01:05.385057   75355 ssh_runner.go:195] Run: sudo systemctl stop -f containerd
I1118 16:01:05.398800   75355 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service containerd
I1118 16:01:05.409450   75355 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I1118 16:01:05.420131   75355 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I1118 16:01:05.435690   75355 ssh_runner.go:195] Run: which cri-dockerd
I1118 16:01:05.438812   75355 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I1118 16:01:05.449461   75355 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (192 bytes)
I1118 16:01:05.466388   75355 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I1118 16:01:05.555852   75355 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I1118 16:01:05.643458   75355 docker.go:575] configuring docker to use "systemd" as cgroup driver...
I1118 16:01:05.643565   75355 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (129 bytes)
I1118 16:01:05.661251   75355 ssh_runner.go:195] Run: sudo systemctl reset-failed docker
I1118 16:01:05.672369   75355 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1118 16:01:05.762532   75355 ssh_runner.go:195] Run: sudo systemctl restart docker
I1118 16:01:07.198291   75355 ssh_runner.go:235] Completed: sudo systemctl restart docker: (1.435728678s)
I1118 16:01:07.198385   75355 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service docker
I1118 16:01:07.209604   75355 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I1118 16:01:07.220782   75355 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I1118 16:01:07.232287   75355 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I1118 16:01:07.323557   75355 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I1118 16:01:07.416082   75355 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1118 16:01:07.497150   75355 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I1118 16:01:07.555693   75355 ssh_runner.go:195] Run: sudo systemctl reset-failed cri-docker.service
I1118 16:01:07.566395   75355 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1118 16:01:07.653795   75355 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I1118 16:01:07.718734   75355 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I1118 16:01:07.729556   75355 start.go:542] Will wait 60s for socket path /var/run/cri-dockerd.sock
I1118 16:01:07.729665   75355 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I1118 16:01:07.732992   75355 start.go:563] Will wait 60s for crictl version
I1118 16:01:07.733060   75355 ssh_runner.go:195] Run: which crictl
I1118 16:01:07.736082   75355 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I1118 16:01:07.767130   75355 start.go:579] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  28.4.0
RuntimeApiVersion:  v1
I1118 16:01:07.767204   75355 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1118 16:01:07.789724   75355 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1118 16:01:07.816846   75355 out.go:252] ðŸ³  Przygotowywanie Kubernetesa v1.34.0 na Docker 28.4.0...
I1118 16:01:07.816965   75355 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I1118 16:01:07.833648   75355 ssh_runner.go:195] Run: grep 192.168.49.1	host.minikube.internal$ /etc/hosts
I1118 16:01:07.837175   75355 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.49.1	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1118 16:01:07.850792   75355 kubeadm.go:875] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 Memory:3900 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s MountString: Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false DisableCoreDNSLog:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I1118 16:01:07.850909   75355 preload.go:131] Checking if preload exists for k8s version v1.34.0 and runtime docker
I1118 16:01:07.850993   75355 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1118 16:01:07.870223   75355 docker.go:691] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.34.0
registry.k8s.io/kube-controller-manager:v1.34.0
registry.k8s.io/kube-proxy:v1.34.0
registry.k8s.io/kube-scheduler:v1.34.0
registry.k8s.io/etcd:3.6.4-0
registry.k8s.io/pause:3.10.1
registry.k8s.io/coredns/coredns:v1.12.1
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I1118 16:01:07.870232   75355 docker.go:621] Images already preloaded, skipping extraction
I1118 16:01:07.870297   75355 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1118 16:01:07.888901   75355 docker.go:691] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.34.0
registry.k8s.io/kube-proxy:v1.34.0
registry.k8s.io/kube-scheduler:v1.34.0
registry.k8s.io/kube-controller-manager:v1.34.0
registry.k8s.io/etcd:3.6.4-0
registry.k8s.io/pause:3.10.1
registry.k8s.io/coredns/coredns:v1.12.1
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I1118 16:01:07.888911   75355 cache_images.go:85] Images are preloaded, skipping loading
I1118 16:01:07.888917   75355 kubeadm.go:926] updating node { 192.168.49.2 8443 v1.34.0 docker true true} ...
I1118 16:01:07.889007   75355 kubeadm.go:938] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.34.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I1118 16:01:07.889072   75355 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I1118 16:01:07.934962   75355 cni.go:84] Creating CNI manager for ""
I1118 16:01:07.934975   75355 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1118 16:01:07.934982   75355 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I1118 16:01:07.934996   75355 kubeadm.go:189] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.34.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:systemd ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I1118 16:01:07.935094   75355 kubeadm.go:195] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta4
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    - name: "node-ip"
      value: "192.168.49.2"
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta4
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    - name: "enable-admission-plugins"
      value: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    - name: "allocate-node-cidrs"
      value: "true"
    - name: "leader-elect"
      value: "false"
scheduler:
  extraArgs:
    - name: "leader-elect"
      value: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
kubernetesVersion: v1.34.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: systemd
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%"
  nodefs.inodesFree: "0%"
  imagefs.available: "0%"
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I1118 16:01:07.935174   75355 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.34.0
I1118 16:01:07.944320   75355 binaries.go:44] Found k8s binaries, skipping transfer
I1118 16:01:07.944397   75355 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I1118 16:01:07.952963   75355 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I1118 16:01:07.970300   75355 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I1118 16:01:07.987822   75355 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2208 bytes)
I1118 16:01:08.004887   75355 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I1118 16:01:08.008073   75355 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1118 16:01:08.018484   75355 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1118 16:01:08.102325   75355 ssh_runner.go:195] Run: sudo systemctl start kubelet
I1118 16:01:08.131696   75355 certs.go:68] Setting up /home/student/.minikube/profiles/minikube for IP: 192.168.49.2
I1118 16:01:08.131705   75355 certs.go:194] generating shared ca certs ...
I1118 16:01:08.131718   75355 certs.go:226] acquiring lock for ca certs: {Name:mk0881d8e71be8b5efe2f2ec2a356f69ba3caa5e Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1118 16:01:08.131809   75355 certs.go:240] generating "minikubeCA" ca cert: /home/student/.minikube/ca.key
I1118 16:01:08.307797   75355 crypto.go:156] Writing cert to /home/student/.minikube/ca.crt ...
I1118 16:01:08.307810   75355 lock.go:35] WriteFile acquiring /home/student/.minikube/ca.crt: {Name:mk85830f802d0e3d9296f68c07bc6290a455eabf Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1118 16:01:08.307971   75355 crypto.go:164] Writing key to /home/student/.minikube/ca.key ...
I1118 16:01:08.307987   75355 lock.go:35] WriteFile acquiring /home/student/.minikube/ca.key: {Name:mkf0ae89a226cd0fcdb7687b61278ee1b1fb8557 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1118 16:01:08.308080   75355 certs.go:240] generating "proxyClientCA" ca cert: /home/student/.minikube/proxy-client-ca.key
I1118 16:01:08.408971   75355 crypto.go:156] Writing cert to /home/student/.minikube/proxy-client-ca.crt ...
I1118 16:01:08.408983   75355 lock.go:35] WriteFile acquiring /home/student/.minikube/proxy-client-ca.crt: {Name:mk1d365cc51aba7d87bede104c948b50907c2ebb Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1118 16:01:08.409119   75355 crypto.go:164] Writing key to /home/student/.minikube/proxy-client-ca.key ...
I1118 16:01:08.409126   75355 lock.go:35] WriteFile acquiring /home/student/.minikube/proxy-client-ca.key: {Name:mk368f8b017c0cc632571b1951211575dd80f522 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1118 16:01:08.409200   75355 certs.go:256] generating profile certs ...
I1118 16:01:08.409246   75355 certs.go:363] generating signed profile cert for "minikube-user": /home/student/.minikube/profiles/minikube/client.key
I1118 16:01:08.409256   75355 crypto.go:68] Generating cert /home/student/.minikube/profiles/minikube/client.crt with IP's: []
I1118 16:01:08.531168   75355 crypto.go:156] Writing cert to /home/student/.minikube/profiles/minikube/client.crt ...
I1118 16:01:08.531182   75355 lock.go:35] WriteFile acquiring /home/student/.minikube/profiles/minikube/client.crt: {Name:mk49e83c789d1fa0db4a82b0a9a4a3ece75b57ad Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1118 16:01:08.531334   75355 crypto.go:164] Writing key to /home/student/.minikube/profiles/minikube/client.key ...
I1118 16:01:08.531340   75355 lock.go:35] WriteFile acquiring /home/student/.minikube/profiles/minikube/client.key: {Name:mk9c9459514852ca99cd61aa0d5349f52d3b8a75 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1118 16:01:08.531423   75355 certs.go:363] generating signed profile cert for "minikube": /home/student/.minikube/profiles/minikube/apiserver.key.7fb57e3c
I1118 16:01:08.531439   75355 crypto.go:68] Generating cert /home/student/.minikube/profiles/minikube/apiserver.crt.7fb57e3c with IP's: [10.96.0.1 127.0.0.1 10.0.0.1 192.168.49.2]
I1118 16:01:08.707040   75355 crypto.go:156] Writing cert to /home/student/.minikube/profiles/minikube/apiserver.crt.7fb57e3c ...
I1118 16:01:08.707054   75355 lock.go:35] WriteFile acquiring /home/student/.minikube/profiles/minikube/apiserver.crt.7fb57e3c: {Name:mk403acdaf34c06acda86b18a32051aafec053de Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1118 16:01:08.707201   75355 crypto.go:164] Writing key to /home/student/.minikube/profiles/minikube/apiserver.key.7fb57e3c ...
I1118 16:01:08.707208   75355 lock.go:35] WriteFile acquiring /home/student/.minikube/profiles/minikube/apiserver.key.7fb57e3c: {Name:mkccf1eb2146e98ac939b358571e166fd77d011c Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1118 16:01:08.707279   75355 certs.go:381] copying /home/student/.minikube/profiles/minikube/apiserver.crt.7fb57e3c -> /home/student/.minikube/profiles/minikube/apiserver.crt
I1118 16:01:08.707365   75355 certs.go:385] copying /home/student/.minikube/profiles/minikube/apiserver.key.7fb57e3c -> /home/student/.minikube/profiles/minikube/apiserver.key
I1118 16:01:08.707422   75355 certs.go:363] generating signed profile cert for "aggregator": /home/student/.minikube/profiles/minikube/proxy-client.key
I1118 16:01:08.707437   75355 crypto.go:68] Generating cert /home/student/.minikube/profiles/minikube/proxy-client.crt with IP's: []
I1118 16:01:08.793575   75355 crypto.go:156] Writing cert to /home/student/.minikube/profiles/minikube/proxy-client.crt ...
I1118 16:01:08.793588   75355 lock.go:35] WriteFile acquiring /home/student/.minikube/profiles/minikube/proxy-client.crt: {Name:mkbe6c23c99417bab5211dd31550402fc4091f3d Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1118 16:01:08.793715   75355 crypto.go:164] Writing key to /home/student/.minikube/profiles/minikube/proxy-client.key ...
I1118 16:01:08.793722   75355 lock.go:35] WriteFile acquiring /home/student/.minikube/profiles/minikube/proxy-client.key: {Name:mk304c034a5abca0de533d936240f28f334cb572 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1118 16:01:08.793939   75355 certs.go:484] found cert: /home/student/.minikube/certs/ca-key.pem (1675 bytes)
I1118 16:01:08.793973   75355 certs.go:484] found cert: /home/student/.minikube/certs/ca.pem (1090 bytes)
I1118 16:01:08.793999   75355 certs.go:484] found cert: /home/student/.minikube/certs/cert.pem (1135 bytes)
I1118 16:01:08.794035   75355 certs.go:484] found cert: /home/student/.minikube/certs/key.pem (1675 bytes)
I1118 16:01:08.794602   75355 ssh_runner.go:362] scp /home/student/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I1118 16:01:08.819484   75355 ssh_runner.go:362] scp /home/student/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I1118 16:01:08.843299   75355 ssh_runner.go:362] scp /home/student/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I1118 16:01:08.867078   75355 ssh_runner.go:362] scp /home/student/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I1118 16:01:08.890238   75355 ssh_runner.go:362] scp /home/student/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I1118 16:01:08.913061   75355 ssh_runner.go:362] scp /home/student/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I1118 16:01:08.936025   75355 ssh_runner.go:362] scp /home/student/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I1118 16:01:08.958499   75355 ssh_runner.go:362] scp /home/student/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1675 bytes)
I1118 16:01:08.981905   75355 ssh_runner.go:362] scp /home/student/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I1118 16:01:09.017328   75355 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I1118 16:01:09.035459   75355 ssh_runner.go:195] Run: openssl version
I1118 16:01:09.040525   75355 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I1118 16:01:09.065355   75355 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I1118 16:01:09.069114   75355 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 Nov 18 15:01 /usr/share/ca-certificates/minikubeCA.pem
I1118 16:01:09.069198   75355 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I1118 16:01:09.075749   75355 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I1118 16:01:09.085102   75355 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I1118 16:01:09.088098   75355 certs.go:399] 'apiserver-kubelet-client' cert doesn't exist, likely first start: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt: Process exited with status 1
stdout:

stderr:
stat: cannot statx '/var/lib/minikube/certs/apiserver-kubelet-client.crt': No such file or directory
I1118 16:01:09.088143   75355 kubeadm.go:392] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 Memory:3900 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s MountString: Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false DisableCoreDNSLog:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I1118 16:01:09.088246   75355 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I1118 16:01:09.105445   75355 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I1118 16:01:09.114526   75355 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I1118 16:01:09.123236   75355 kubeadm.go:214] ignoring SystemVerification for kubeadm because of docker driver
I1118 16:01:09.123304   75355 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I1118 16:01:09.131893   75355 kubeadm.go:155] config check failed, skipping stale config cleanup: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
ls: cannot access '/etc/kubernetes/admin.conf': No such file or directory
ls: cannot access '/etc/kubernetes/kubelet.conf': No such file or directory
ls: cannot access '/etc/kubernetes/controller-manager.conf': No such file or directory
ls: cannot access '/etc/kubernetes/scheduler.conf': No such file or directory
I1118 16:01:09.131900   75355 kubeadm.go:157] found existing configuration files:

I1118 16:01:09.131956   75355 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I1118 16:01:09.140335   75355 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/admin.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/admin.conf: No such file or directory
I1118 16:01:09.140404   75355 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/admin.conf
I1118 16:01:09.148745   75355 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I1118 16:01:09.157360   75355 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/kubelet.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/kubelet.conf: No such file or directory
I1118 16:01:09.157427   75355 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/kubelet.conf
I1118 16:01:09.165573   75355 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I1118 16:01:09.173969   75355 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/controller-manager.conf: No such file or directory
I1118 16:01:09.174021   75355 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I1118 16:01:09.182104   75355 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I1118 16:01:09.190707   75355 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/scheduler.conf: No such file or directory
I1118 16:01:09.190783   75355 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I1118 16:01:09.198931   75355 ssh_runner.go:286] Start: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.34.0:$PATH" kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,NumCPU,Mem,SystemVerification,FileContent--proc-sys-net-bridge-bridge-nf-call-iptables"
I1118 16:01:09.233522   75355 kubeadm.go:310] [init] Using Kubernetes version: v1.34.0
I1118 16:01:09.233577   75355 kubeadm.go:310] [preflight] Running pre-flight checks
I1118 16:01:09.250149   75355 kubeadm.go:310] [preflight] The system verification failed. Printing the output from the verification:
I1118 16:01:09.250200   75355 kubeadm.go:310] [0;37mKERNEL_VERSION[0m: [0;32m6.12.48+deb13-amd64[0m
I1118 16:01:09.250237   75355 kubeadm.go:310] [0;37mOS[0m: [0;32mLinux[0m
I1118 16:01:09.250283   75355 kubeadm.go:310] [0;37mCGROUPS_CPU[0m: [0;32menabled[0m
I1118 16:01:09.250316   75355 kubeadm.go:310] [0;37mCGROUPS_CPUSET[0m: [0;32menabled[0m
I1118 16:01:09.250356   75355 kubeadm.go:310] [0;37mCGROUPS_DEVICES[0m: [0;32menabled[0m
I1118 16:01:09.250403   75355 kubeadm.go:310] [0;37mCGROUPS_FREEZER[0m: [0;32menabled[0m
I1118 16:01:09.250436   75355 kubeadm.go:310] [0;37mCGROUPS_MEMORY[0m: [0;32menabled[0m
I1118 16:01:09.250476   75355 kubeadm.go:310] [0;37mCGROUPS_PIDS[0m: [0;32menabled[0m
I1118 16:01:09.250510   75355 kubeadm.go:310] [0;37mCGROUPS_HUGETLB[0m: [0;32menabled[0m
I1118 16:01:09.250563   75355 kubeadm.go:310] [0;37mCGROUPS_IO[0m: [0;32menabled[0m
I1118 16:01:09.299955   75355 kubeadm.go:310] [preflight] Pulling images required for setting up a Kubernetes cluster
I1118 16:01:09.300039   75355 kubeadm.go:310] [preflight] This might take a minute or two, depending on the speed of your internet connection
I1118 16:01:09.300103   75355 kubeadm.go:310] [preflight] You can also perform this action beforehand using 'kubeadm config images pull'
I1118 16:01:09.309172   75355 kubeadm.go:310] [certs] Using certificateDir folder "/var/lib/minikube/certs"
I1118 16:01:09.317720   75355 out.go:252]     â–ª Generating certificates and keys ...
I1118 16:01:09.317813   75355 kubeadm.go:310] [certs] Using existing ca certificate authority
I1118 16:01:09.317860   75355 kubeadm.go:310] [certs] Using existing apiserver certificate and key on disk
I1118 16:01:09.681528   75355 kubeadm.go:310] [certs] Generating "apiserver-kubelet-client" certificate and key
I1118 16:01:09.864890   75355 kubeadm.go:310] [certs] Generating "front-proxy-ca" certificate and key
I1118 16:01:09.991508   75355 kubeadm.go:310] [certs] Generating "front-proxy-client" certificate and key
I1118 16:01:10.173222   75355 kubeadm.go:310] [certs] Generating "etcd/ca" certificate and key
I1118 16:01:10.358200   75355 kubeadm.go:310] [certs] Generating "etcd/server" certificate and key
I1118 16:01:10.358291   75355 kubeadm.go:310] [certs] etcd/server serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I1118 16:01:10.473398   75355 kubeadm.go:310] [certs] Generating "etcd/peer" certificate and key
I1118 16:01:10.473493   75355 kubeadm.go:310] [certs] etcd/peer serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I1118 16:01:10.671340   75355 kubeadm.go:310] [certs] Generating "etcd/healthcheck-client" certificate and key
I1118 16:01:11.089050   75355 kubeadm.go:310] [certs] Generating "apiserver-etcd-client" certificate and key
I1118 16:01:11.474929   75355 kubeadm.go:310] [certs] Generating "sa" key and public key
I1118 16:01:11.474975   75355 kubeadm.go:310] [kubeconfig] Using kubeconfig folder "/etc/kubernetes"
I1118 16:01:11.713554   75355 kubeadm.go:310] [kubeconfig] Writing "admin.conf" kubeconfig file
I1118 16:01:12.044486   75355 kubeadm.go:310] [kubeconfig] Writing "super-admin.conf" kubeconfig file
I1118 16:01:12.245460   75355 kubeadm.go:310] [kubeconfig] Writing "kubelet.conf" kubeconfig file
I1118 16:01:12.917709   75355 kubeadm.go:310] [kubeconfig] Writing "controller-manager.conf" kubeconfig file
I1118 16:01:13.014450   75355 kubeadm.go:310] [kubeconfig] Writing "scheduler.conf" kubeconfig file
I1118 16:01:13.014677   75355 kubeadm.go:310] [etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
I1118 16:01:13.021206   75355 kubeadm.go:310] [control-plane] Using manifest folder "/etc/kubernetes/manifests"
I1118 16:01:13.039061   75355 out.go:252]     â–ª Uruchamianie pÅ‚aszczyzny kontrolnej ...
I1118 16:01:13.039167   75355 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-apiserver"
I1118 16:01:13.039217   75355 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-controller-manager"
I1118 16:01:13.039266   75355 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-scheduler"
I1118 16:01:13.039333   75355 kubeadm.go:310] [kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
I1118 16:01:13.039399   75355 kubeadm.go:310] [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/instance-config.yaml"
I1118 16:01:13.043856   75355 kubeadm.go:310] [patches] Applied patch of type "application/strategic-merge-patch+json" to target "kubeletconfiguration"
I1118 16:01:13.043941   75355 kubeadm.go:310] [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
I1118 16:01:13.043977   75355 kubeadm.go:310] [kubelet-start] Starting the kubelet
I1118 16:01:13.163044   75355 kubeadm.go:310] [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests"
I1118 16:01:13.163129   75355 kubeadm.go:310] [kubelet-check] Waiting for a healthy kubelet at http://127.0.0.1:10248/healthz. This can take up to 4m0s
I1118 16:01:14.163824   75355 kubeadm.go:310] [kubelet-check] The kubelet is healthy after 1.000957317s
I1118 16:01:14.166098   75355 kubeadm.go:310] [control-plane-check] Waiting for healthy control plane components. This can take up to 4m0s
I1118 16:01:14.166173   75355 kubeadm.go:310] [control-plane-check] Checking kube-apiserver at https://192.168.49.2:8443/livez
I1118 16:01:14.166240   75355 kubeadm.go:310] [control-plane-check] Checking kube-controller-manager at https://127.0.0.1:10257/healthz
I1118 16:01:14.166298   75355 kubeadm.go:310] [control-plane-check] Checking kube-scheduler at https://127.0.0.1:10259/livez
I1118 16:01:16.441012   75355 kubeadm.go:310] [control-plane-check] kube-controller-manager is healthy after 2.273185711s
I1118 16:01:16.522995   75355 kubeadm.go:310] [control-plane-check] kube-scheduler is healthy after 2.356888563s
I1118 16:01:18.167270   75355 kubeadm.go:310] [control-plane-check] kube-apiserver is healthy after 4.001056759s
I1118 16:01:18.178499   75355 kubeadm.go:310] [upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
I1118 16:01:18.188964   75355 kubeadm.go:310] [kubelet] Creating a ConfigMap "kubelet-config" in namespace kube-system with the configuration for the kubelets in the cluster
I1118 16:01:18.196452   75355 kubeadm.go:310] [upload-certs] Skipping phase. Please see --upload-certs
I1118 16:01:18.196625   75355 kubeadm.go:310] [mark-control-plane] Marking the node minikube as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
I1118 16:01:18.205357   75355 kubeadm.go:310] [bootstrap-token] Using token: 77uarm.iigo9kocq3wgxaxr
I1118 16:01:18.206948   75355 out.go:252]     â–ª Konfigurowanie zasad RBAC ...
I1118 16:01:18.207098   75355 kubeadm.go:310] [bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
I1118 16:01:18.209657   75355 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes
I1118 16:01:18.216059   75355 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
I1118 16:01:18.218279   75355 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
I1118 16:01:18.220515   75355 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
I1118 16:01:18.223172   75355 kubeadm.go:310] [bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
I1118 16:01:18.573412   75355 kubeadm.go:310] [kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
I1118 16:01:18.988921   75355 kubeadm.go:310] [addons] Applied essential addon: CoreDNS
I1118 16:01:19.575591   75355 kubeadm.go:310] [addons] Applied essential addon: kube-proxy
I1118 16:01:19.576437   75355 kubeadm.go:310] 
I1118 16:01:19.576522   75355 kubeadm.go:310] Your Kubernetes control-plane has initialized successfully!
I1118 16:01:19.576527   75355 kubeadm.go:310] 
I1118 16:01:19.576639   75355 kubeadm.go:310] To start using your cluster, you need to run the following as a regular user:
I1118 16:01:19.576643   75355 kubeadm.go:310] 
I1118 16:01:19.576673   75355 kubeadm.go:310]   mkdir -p $HOME/.kube
I1118 16:01:19.576763   75355 kubeadm.go:310]   sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
I1118 16:01:19.576831   75355 kubeadm.go:310]   sudo chown $(id -u):$(id -g) $HOME/.kube/config
I1118 16:01:19.576839   75355 kubeadm.go:310] 
I1118 16:01:19.576879   75355 kubeadm.go:310] Alternatively, if you are the root user, you can run:
I1118 16:01:19.576882   75355 kubeadm.go:310] 
I1118 16:01:19.576915   75355 kubeadm.go:310]   export KUBECONFIG=/etc/kubernetes/admin.conf
I1118 16:01:19.576918   75355 kubeadm.go:310] 
I1118 16:01:19.576975   75355 kubeadm.go:310] You should now deploy a pod network to the cluster.
I1118 16:01:19.577100   75355 kubeadm.go:310] Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
I1118 16:01:19.577209   75355 kubeadm.go:310]   https://kubernetes.io/docs/concepts/cluster-administration/addons/
I1118 16:01:19.577214   75355 kubeadm.go:310] 
I1118 16:01:19.577349   75355 kubeadm.go:310] You can now join any number of control-plane nodes by copying certificate authorities
I1118 16:01:19.577453   75355 kubeadm.go:310] and service account keys on each node and then running the following as root:
I1118 16:01:19.577457   75355 kubeadm.go:310] 
I1118 16:01:19.577550   75355 kubeadm.go:310]   kubeadm join control-plane.minikube.internal:8443 --token 77uarm.iigo9kocq3wgxaxr \
I1118 16:01:19.577625   75355 kubeadm.go:310] 	--discovery-token-ca-cert-hash sha256:c41f0270cfd99316a9a57faccc5e5b353d1e637461375861f5053731a96661a8 \
I1118 16:01:19.577645   75355 kubeadm.go:310] 	--control-plane 
I1118 16:01:19.577648   75355 kubeadm.go:310] 
I1118 16:01:19.577709   75355 kubeadm.go:310] Then you can join any number of worker nodes by running the following on each as root:
I1118 16:01:19.577711   75355 kubeadm.go:310] 
I1118 16:01:19.577775   75355 kubeadm.go:310] kubeadm join control-plane.minikube.internal:8443 --token 77uarm.iigo9kocq3wgxaxr \
I1118 16:01:19.577853   75355 kubeadm.go:310] 	--discovery-token-ca-cert-hash sha256:c41f0270cfd99316a9a57faccc5e5b353d1e637461375861f5053731a96661a8 
I1118 16:01:19.579865   75355 kubeadm.go:310] 	[WARNING Swap]: swap is supported for cgroup v2 only. The kubelet must be properly configured to use swap. Please refer to https://kubernetes.io/docs/concepts/architecture/nodes/#swap-memory, or disable swap on the node
I1118 16:01:19.580032   75355 kubeadm.go:310] 	[WARNING SystemVerification]: failed to parse kernel config: unable to load kernel module: "configs", output: "modprobe: FATAL: Module configs not found in directory /lib/modules/6.12.48+deb13-amd64\n", err: exit status 1
I1118 16:01:19.580124   75355 kubeadm.go:310] 	[WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
I1118 16:01:19.580139   75355 cni.go:84] Creating CNI manager for ""
I1118 16:01:19.580151   75355 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1118 16:01:19.581854   75355 out.go:179] ðŸ”—  Configuring bridge CNI (Container Networking Interface) ...
I1118 16:01:19.583400   75355 ssh_runner.go:195] Run: sudo mkdir -p /etc/cni/net.d
I1118 16:01:19.593263   75355 ssh_runner.go:362] scp memory --> /etc/cni/net.d/1-k8s.conflist (496 bytes)
I1118 16:01:19.611433   75355 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I1118 16:01:19.611528   75355 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.34.0/kubectl create clusterrolebinding minikube-rbac --clusterrole=cluster-admin --serviceaccount=kube-system:default --kubeconfig=/var/lib/minikube/kubeconfig
I1118 16:01:19.611560   75355 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.34.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig label --overwrite nodes minikube minikube.k8s.io/updated_at=2025_11_18T16_01_19_0700 minikube.k8s.io/version=v1.37.0 minikube.k8s.io/commit=65318f4cfff9c12cc87ec9eb8f4cdd57b25047f3 minikube.k8s.io/name=minikube minikube.k8s.io/primary=true
I1118 16:01:19.619268   75355 ops.go:34] apiserver oom_adj: -16
I1118 16:01:19.721529   75355 kubeadm.go:1105] duration metric: took 110.075329ms to wait for elevateKubeSystemPrivileges
I1118 16:01:19.747301   75355 kubeadm.go:394] duration metric: took 10.659155702s to StartCluster
I1118 16:01:19.747324   75355 settings.go:142] acquiring lock: {Name:mk7b840ed4aabf08d3f9dac61666174bc6a46f81 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1118 16:01:19.747399   75355 settings.go:150] Updating kubeconfig:  /home/student/.kube/config
I1118 16:01:19.747809   75355 lock.go:35] WriteFile acquiring /home/student/.kube/config: {Name:mk6d365e1247d817ae1f5dfa9e982ee22a78f17a Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1118 16:01:19.747977   75355 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.34.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I1118 16:01:19.747984   75355 start.go:235] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I1118 16:01:19.748089   75355 addons.go:511] enable addons start: toEnable=map[ambassador:false amd-gpu-device-plugin:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubetail:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volcano:false volumesnapshots:false yakd:false]
I1118 16:01:19.748166   75355 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I1118 16:01:19.748175   75355 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.34.0
I1118 16:01:19.748183   75355 addons.go:69] Setting default-storageclass=true in profile "minikube"
I1118 16:01:19.748196   75355 addons.go:238] Setting addon storage-provisioner=true in "minikube"
I1118 16:01:19.748210   75355 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I1118 16:01:19.748235   75355 host.go:66] Checking if "minikube" exists ...
I1118 16:01:19.748576   75355 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1118 16:01:19.748700   75355 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1118 16:01:19.749643   75355 out.go:179] ðŸ”Ž  Verifying Kubernetes components...
I1118 16:01:19.751245   75355 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1118 16:01:19.772080   75355 out.go:179]     â–ª Using image gcr.io/k8s-minikube/storage-provisioner:v5
I1118 16:01:19.773576   75355 addons.go:435] installing /etc/kubernetes/addons/storage-provisioner.yaml
I1118 16:01:19.773584   75355 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I1118 16:01:19.773675   75355 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1118 16:01:19.778444   75355 addons.go:238] Setting addon default-storageclass=true in "minikube"
I1118 16:01:19.778470   75355 host.go:66] Checking if "minikube" exists ...
I1118 16:01:19.778925   75355 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1118 16:01:19.792708   75355 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/student/.minikube/machines/minikube/id_rsa Username:docker}
I1118 16:01:19.800405   75355 addons.go:435] installing /etc/kubernetes/addons/storageclass.yaml
I1118 16:01:19.800432   75355 ssh_runner.go:362] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I1118 16:01:19.800533   75355 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1118 16:01:19.821785   75355 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.34.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml | sed -e '/^        forward . \/etc\/resolv.conf.*/i \        hosts {\n           192.168.49.1 host.minikube.internal\n           fallthrough\n        }' -e '/^        errors *$/i \        log' | sudo /var/lib/minikube/binaries/v1.34.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig replace -f -"
I1118 16:01:19.823691   75355 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/student/.minikube/machines/minikube/id_rsa Username:docker}
I1118 16:01:19.881032   75355 ssh_runner.go:195] Run: sudo systemctl start kubelet
I1118 16:01:19.911359   75355 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I1118 16:01:19.935345   75355 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I1118 16:01:19.970593   75355 start.go:976] {"host.minikube.internal": 192.168.49.1} host record injected into CoreDNS's ConfigMap
I1118 16:01:19.971851   75355 api_server.go:52] waiting for apiserver process to appear ...
I1118 16:01:19.971925   75355 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1118 16:01:20.130210   75355 api_server.go:72] duration metric: took 382.204156ms to wait for apiserver process to appear ...
I1118 16:01:20.130223   75355 api_server.go:88] waiting for apiserver healthz status ...
I1118 16:01:20.130238   75355 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1118 16:01:20.133618   75355 api_server.go:279] https://192.168.49.2:8443/healthz returned 200:
ok
I1118 16:01:20.134221   75355 api_server.go:141] control plane version: v1.34.0
I1118 16:01:20.134233   75355 api_server.go:131] duration metric: took 4.005979ms to wait for apiserver health ...
I1118 16:01:20.134238   75355 system_pods.go:43] waiting for kube-system pods to appear ...
I1118 16:01:20.137182   75355 out.go:179] ðŸŒŸ  Enabled addons: storage-provisioner, default-storageclass
I1118 16:01:20.137717   75355 system_pods.go:59] 5 kube-system pods found
I1118 16:01:20.137778   75355 system_pods.go:61] "etcd-minikube" [b421ec13-29d2-4be5-8e8b-2b0f2b54476b] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I1118 16:01:20.137794   75355 system_pods.go:61] "kube-apiserver-minikube" [793364c9-ab92-4c7d-a470-662ef9d49763] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I1118 16:01:20.137807   75355 system_pods.go:61] "kube-controller-manager-minikube" [e4d7de29-e2a9-495b-b5ef-349b3cb90735] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I1118 16:01:20.137817   75355 system_pods.go:61] "kube-scheduler-minikube" [9a77b5ee-a9e7-431a-b629-ff2f7d21bf43] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I1118 16:01:20.137824   75355 system_pods.go:61] "storage-provisioner" [843fc0d7-b3be-46f5-b05d-5a0a154e2c05] Pending
I1118 16:01:20.137833   75355 system_pods.go:74] duration metric: took 3.58768ms to wait for pod list to return data ...
I1118 16:01:20.137850   75355 kubeadm.go:578] duration metric: took 389.843086ms to wait for: map[apiserver:true system_pods:true]
I1118 16:01:20.137867   75355 node_conditions.go:102] verifying NodePressure condition ...
I1118 16:01:20.139796   75355 addons.go:514] duration metric: took 391.751364ms for enable addons: enabled=[storage-provisioner default-storageclass]
I1118 16:01:20.139824   75355 node_conditions.go:122] node storage ephemeral capacity is 97341748Ki
I1118 16:01:20.139834   75355 node_conditions.go:123] node cpu capacity is 8
I1118 16:01:20.139843   75355 node_conditions.go:105] duration metric: took 1.97201ms to run NodePressure ...
I1118 16:01:20.139851   75355 start.go:241] waiting for startup goroutines ...
I1118 16:01:20.474371   75355 kapi.go:214] "coredns" deployment in "kube-system" namespace and "minikube" context rescaled to 1 replicas
I1118 16:01:20.474397   75355 start.go:246] waiting for cluster config update ...
I1118 16:01:20.474415   75355 start.go:255] writing updated cluster config ...
I1118 16:01:20.474713   75355 ssh_runner.go:195] Run: rm -f paused
I1118 16:01:20.624138   75355 start.go:617] kubectl: 1.34.1, cluster: 1.34.0 (minor skew: 0)
I1118 16:01:20.628423   75355 out.go:179] ðŸ„  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
Nov 18 15:01:05 minikube dockerd[1116]: time="2025-11-18T15:01:05.878700736Z" level=info msg="CDI directory does not exist, skipping: failed to monitor for changes: no such file or directory" dir=/var/run/cdi
Nov 18 15:01:05 minikube dockerd[1116]: time="2025-11-18T15:01:05.878720604Z" level=info msg="CDI directory does not exist, skipping: failed to monitor for changes: no such file or directory" dir=/etc/cdi
Nov 18 15:01:05 minikube dockerd[1116]: time="2025-11-18T15:01:05.888370067Z" level=info msg="Creating a containerd client" address=/run/containerd/containerd.sock timeout=1m0s
Nov 18 15:01:05 minikube dockerd[1116]: time="2025-11-18T15:01:05.906685347Z" level=info msg="[graphdriver] trying configured driver: overlay2"
Nov 18 15:01:05 minikube dockerd[1116]: time="2025-11-18T15:01:05.947618984Z" level=info msg="Loading containers: start."
Nov 18 15:01:07 minikube dockerd[1116]: time="2025-11-18T15:01:07.082689493Z" level=warning msg="Error (Unable to complete atomic operation, key modified) deleting object [endpoint_count 76c0d4eff3f05bc16a5651292ba0c340534b88ca4cf14bcafd8726c103ec33b2], retrying...."
Nov 18 15:01:07 minikube dockerd[1116]: time="2025-11-18T15:01:07.145100143Z" level=info msg="Loading containers: done."
Nov 18 15:01:07 minikube dockerd[1116]: time="2025-11-18T15:01:07.168474281Z" level=info msg="Docker daemon" commit=249d679 containerd-snapshotter=false storage-driver=overlay2 version=28.4.0
Nov 18 15:01:07 minikube dockerd[1116]: time="2025-11-18T15:01:07.168524803Z" level=info msg="Initializing buildkit"
Nov 18 15:01:07 minikube dockerd[1116]: time="2025-11-18T15:01:07.190658780Z" level=info msg="Completed buildkit initialization"
Nov 18 15:01:07 minikube dockerd[1116]: time="2025-11-18T15:01:07.196529161Z" level=info msg="Daemon has completed initialization"
Nov 18 15:01:07 minikube dockerd[1116]: time="2025-11-18T15:01:07.196597229Z" level=info msg="API listen on /var/run/docker.sock"
Nov 18 15:01:07 minikube dockerd[1116]: time="2025-11-18T15:01:07.196668261Z" level=info msg="API listen on [::]:2376"
Nov 18 15:01:07 minikube dockerd[1116]: time="2025-11-18T15:01:07.196608365Z" level=info msg="API listen on /run/docker.sock"
Nov 18 15:01:07 minikube systemd[1]: Started Docker Application Container Engine.
Nov 18 15:01:07 minikube systemd[1]: Starting CRI Interface for Docker Application Container Engine...
Nov 18 15:01:07 minikube cri-dockerd[1420]: time="2025-11-18T15:01:07Z" level=info msg="Starting cri-dockerd dev (HEAD)"
Nov 18 15:01:07 minikube cri-dockerd[1420]: time="2025-11-18T15:01:07Z" level=info msg="Connecting to docker on the Endpoint unix:///var/run/docker.sock"
Nov 18 15:01:07 minikube cri-dockerd[1420]: time="2025-11-18T15:01:07Z" level=info msg="Start docker client with request timeout 0s"
Nov 18 15:01:07 minikube cri-dockerd[1420]: time="2025-11-18T15:01:07Z" level=info msg="Hairpin mode is set to hairpin-veth"
Nov 18 15:01:07 minikube cri-dockerd[1420]: time="2025-11-18T15:01:07Z" level=info msg="Loaded network plugin cni"
Nov 18 15:01:07 minikube cri-dockerd[1420]: time="2025-11-18T15:01:07Z" level=info msg="Docker cri networking managed by network plugin cni"
Nov 18 15:01:07 minikube cri-dockerd[1420]: time="2025-11-18T15:01:07Z" level=info msg="Setting cgroupDriver systemd"
Nov 18 15:01:07 minikube cri-dockerd[1420]: time="2025-11-18T15:01:07Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:,},}"
Nov 18 15:01:07 minikube cri-dockerd[1420]: time="2025-11-18T15:01:07Z" level=info msg="Starting the GRPC backend for the Docker CRI interface."
Nov 18 15:01:07 minikube cri-dockerd[1420]: time="2025-11-18T15:01:07Z" level=info msg="Start cri-dockerd grpc backend"
Nov 18 15:01:07 minikube systemd[1]: Started CRI Interface for Docker Application Container Engine.
Nov 18 15:01:14 minikube cri-dockerd[1420]: time="2025-11-18T15:01:14Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/074b1993d5e393e52849cfca5e6a82e0584d4e929d9ca5bdcfcc968864858e06/resolv.conf as [nameserver 192.168.49.1 options ndots:0]"
Nov 18 15:01:14 minikube cri-dockerd[1420]: time="2025-11-18T15:01:14Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/5382d9e7fcc601460bffa1e97180512dc7ad9634956804c63ce9eee8be8b64ab/resolv.conf as [nameserver 192.168.49.1 options ndots:0]"
Nov 18 15:01:14 minikube cri-dockerd[1420]: time="2025-11-18T15:01:14Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/377b960df24f83f41813e3d80d9ef8ef36e9dcb6fe09f8af7bdf95543655ac28/resolv.conf as [nameserver 192.168.49.1 options ndots:0]"
Nov 18 15:01:14 minikube cri-dockerd[1420]: time="2025-11-18T15:01:14Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/f2558fa1b7a4270cd14e9be1fbb047989a1369ebf88578ba231f5958ade6533b/resolv.conf as [nameserver 192.168.49.1 options ndots:0]"
Nov 18 15:01:24 minikube cri-dockerd[1420]: time="2025-11-18T15:01:24Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/68d782399e54e3e54a4f8ab22319af8a70a415d90265098517b5bd79f4cd4aca/resolv.conf as [nameserver 192.168.49.1 options ndots:0]"
Nov 18 15:01:25 minikube cri-dockerd[1420]: time="2025-11-18T15:01:25Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/41d032217a274408af16615b30c33965b779295c5459d32a89aeae8f658b6d2e/resolv.conf as [nameserver 192.168.49.1 options ndots:0]"
Nov 18 15:01:25 minikube cri-dockerd[1420]: time="2025-11-18T15:01:25Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/fa3279035eb3a493c4dad32c4f72c26bfc5b596877c5e79db19049fd100280e1/resolv.conf as [nameserver 192.168.49.1 options ndots:0]"
Nov 18 15:01:29 minikube cri-dockerd[1420]: time="2025-11-18T15:01:29Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:10.244.0.0/24,},}"
Nov 18 15:19:59 minikube cri-dockerd[1420]: time="2025-11-18T15:19:59Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/be2223ffe29230343eb0df5c3742edb386b5589a1b71b075c56e7e964c7b7fea/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Nov 18 15:20:00 minikube dockerd[1116]: time="2025-11-18T15:20:00.627171737Z" level=error msg="Not continuing with pull after error" error="errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Nov 18 15:20:00 minikube dockerd[1116]: time="2025-11-18T15:20:00.627219586Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Nov 18 15:20:14 minikube dockerd[1116]: time="2025-11-18T15:20:14.232072871Z" level=error msg="Not continuing with pull after error" error="errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Nov 18 15:20:14 minikube dockerd[1116]: time="2025-11-18T15:20:14.232128248Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Nov 18 15:20:39 minikube dockerd[1116]: time="2025-11-18T15:20:39.252950614Z" level=error msg="Not continuing with pull after error" error="errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Nov 18 15:20:39 minikube dockerd[1116]: time="2025-11-18T15:20:39.253006665Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Nov 18 15:21:31 minikube dockerd[1116]: time="2025-11-18T15:21:31.242850027Z" level=error msg="Not continuing with pull after error" error="errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Nov 18 15:21:31 minikube dockerd[1116]: time="2025-11-18T15:21:31.242910399Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Nov 18 15:23:02 minikube dockerd[1116]: time="2025-11-18T15:23:02.257384767Z" level=error msg="Not continuing with pull after error" error="errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Nov 18 15:23:02 minikube dockerd[1116]: time="2025-11-18T15:23:02.257444577Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Nov 18 15:24:02 minikube dockerd[1116]: time="2025-11-18T15:24:02.746219406Z" level=error msg="Not continuing with pull after error" error="errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Nov 18 15:24:02 minikube dockerd[1116]: time="2025-11-18T15:24:02.746339304Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Nov 18 15:25:00 minikube dockerd[1116]: time="2025-11-18T15:25:00.509512223Z" level=info msg="ignoring event" container=be2223ffe29230343eb0df5c3742edb386b5589a1b71b075c56e7e964c7b7fea module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 18 15:25:02 minikube cri-dockerd[1420]: time="2025-11-18T15:25:02Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/3e5363c47092a42de13641bd1a135df3bf212cf882a715a5e0955bc2c4fa1003/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Nov 18 15:25:03 minikube dockerd[1116]: time="2025-11-18T15:25:03.883274240Z" level=error msg="Not continuing with pull after error" error="errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Nov 18 15:25:03 minikube dockerd[1116]: time="2025-11-18T15:25:03.883322496Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Nov 18 15:25:20 minikube dockerd[1116]: time="2025-11-18T15:25:20.253658297Z" level=error msg="Not continuing with pull after error" error="errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Nov 18 15:25:20 minikube dockerd[1116]: time="2025-11-18T15:25:20.253700135Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Nov 18 15:25:47 minikube dockerd[1116]: time="2025-11-18T15:25:47.222037031Z" level=error msg="Not continuing with pull after error" error="errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Nov 18 15:25:47 minikube dockerd[1116]: time="2025-11-18T15:25:47.222081785Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Nov 18 15:26:39 minikube dockerd[1116]: time="2025-11-18T15:26:39.242737772Z" level=error msg="Not continuing with pull after error" error="errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Nov 18 15:26:39 minikube dockerd[1116]: time="2025-11-18T15:26:39.242774187Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Nov 18 15:26:43 minikube dockerd[1116]: time="2025-11-18T15:26:43.837805450Z" level=info msg="ignoring event" container=3e5363c47092a42de13641bd1a135df3bf212cf882a715a5e0955bc2c4fa1003 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 18 15:26:46 minikube cri-dockerd[1420]: time="2025-11-18T15:26:46Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/84aef69b15b72d2100302b2109205c109448ff91c69cc19436ed89cd801381eb/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"


==> container status <==
CONTAINER           IMAGE               CREATED             STATE               NAME                      ATTEMPT             POD ID              POD
88f72df107d94       87f5543f0c943       5 minutes ago       Running             fastapi                   0                   84aef69b15b72       fastapi-pod
8ca16d1792df5       6e38f40d628db       31 minutes ago      Running             storage-provisioner       0                   fa3279035eb3a       storage-provisioner
92faa854dc83d       52546a367cc9e       31 minutes ago      Running             coredns                   0                   41d032217a274       coredns-66bc5c9577-c98bj
5d921d66a0748       df0860106674d       31 minutes ago      Running             kube-proxy                0                   68d782399e54e       kube-proxy-2qbt7
4cdaa8a22af7c       a0af72f2ec6d6       31 minutes ago      Running             kube-controller-manager   0                   377b960df24f8       kube-controller-manager-minikube
a473f83685d6c       46169d968e920       31 minutes ago      Running             kube-scheduler            0                   f2558fa1b7a42       kube-scheduler-minikube
8ad509116fdf8       90550c43ad2bc       31 minutes ago      Running             kube-apiserver            0                   5382d9e7fcc60       kube-apiserver-minikube
b8bb42f78de99       5f1f5298c888d       31 minutes ago      Running             etcd                      0                   074b1993d5e39       etcd-minikube


==> coredns [92faa854dc83] <==
maxprocs: Leaving GOMAXPROCS=8: CPU quota undefined
.:53
[INFO] plugin/reload: Running configuration SHA512 = 9e2996f8cb67ac53e0259ab1f8d615d07d1beb0bd07e6a1e39769c3bf486a905bb991cc47f8d2f14d0d3a90a87dfc625a0b4c524fed169d8158c40657c0694b1
CoreDNS-1.12.1
linux/amd64, go1.24.1, 707c7c1
[INFO] 127.0.0.1:58303 - 12103 "HINFO IN 2012042965643868238.6555930383903507150. udp 57 false 512" NXDOMAIN qr,rd,ra 132 0.03651178s


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=65318f4cfff9c12cc87ec9eb8f4cdd57b25047f3
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2025_11_18T16_01_19_0700
                    minikube.k8s.io/version=v1.37.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Tue, 18 Nov 2025 15:01:16 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Tue, 18 Nov 2025 15:32:26 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Tue, 18 Nov 2025 15:30:15 +0000   Tue, 18 Nov 2025 15:01:16 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Tue, 18 Nov 2025 15:30:15 +0000   Tue, 18 Nov 2025 15:01:16 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Tue, 18 Nov 2025 15:30:15 +0000   Tue, 18 Nov 2025 15:01:16 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Tue, 18 Nov 2025 15:30:15 +0000   Tue, 18 Nov 2025 15:01:17 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                8
  ephemeral-storage:  97341748Ki
  hugepages-2Mi:      0
  memory:             16382156Ki
  pods:               110
Allocatable:
  cpu:                8
  ephemeral-storage:  97341748Ki
  hugepages-2Mi:      0
  memory:             16382156Ki
  pods:               110
System Info:
  Machine ID:                 4413060b38034ac5af934affcaabfd99
  System UUID:                ade213d5-d49d-406f-90b9-89b58749a1ae
  Boot ID:                    53745885-6b92-41bb-b821-23286ff99d82
  Kernel Version:             6.12.48+deb13-amd64
  OS Image:                   Ubuntu 22.04.5 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://28.4.0
  Kubelet Version:            v1.34.0
  Kube-Proxy Version:         
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (8 in total)
  Namespace                   Name                                CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                ------------  ----------  ---------------  -------------  ---
  default                     fastapi-pod                         0 (0%)        0 (0%)      0 (0%)           0 (0%)         5m48s
  kube-system                 coredns-66bc5c9577-c98bj            100m (1%)     0 (0%)      70Mi (0%)        170Mi (1%)     31m
  kube-system                 etcd-minikube                       100m (1%)     0 (0%)      100Mi (0%)       0 (0%)         31m
  kube-system                 kube-apiserver-minikube             250m (3%)     0 (0%)      0 (0%)           0 (0%)         31m
  kube-system                 kube-controller-manager-minikube    200m (2%)     0 (0%)      0 (0%)           0 (0%)         31m
  kube-system                 kube-proxy-2qbt7                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         31m
  kube-system                 kube-scheduler-minikube             100m (1%)     0 (0%)      0 (0%)           0 (0%)         31m
  kube-system                 storage-provisioner                 0 (0%)        0 (0%)      0 (0%)           0 (0%)         31m
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                750m (9%)   0 (0%)
  memory             170Mi (1%)  170Mi (1%)
  ephemeral-storage  0 (0%)      0 (0%)
  hugepages-2Mi      0 (0%)      0 (0%)
Events:
  Type    Reason                   Age                From             Message
  ----    ------                   ----               ----             -------
  Normal  Starting                 31m                kube-proxy       
  Normal  Starting                 31m                kubelet          Starting kubelet.
  Normal  NodeHasSufficientMemory  31m (x8 over 31m)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    31m (x8 over 31m)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     31m (x7 over 31m)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  NodeAllocatableEnforced  31m                kubelet          Updated Node Allocatable limit across pods
  Normal  Starting                 31m                kubelet          Starting kubelet.
  Normal  NodeAllocatableEnforced  31m                kubelet          Updated Node Allocatable limit across pods
  Normal  NodeHasSufficientMemory  31m                kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    31m                kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     31m                kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  RegisteredNode           31m                node-controller  Node minikube event: Registered Node minikube in Controller


==> dmesg <==
[  +0.000032] [UFW BLOCK] IN=ens18 OUT= MAC=02:aa:01:ee:fa:ec:02:aa:01:ea:46:06:08:00 SRC=192.168.100.28 DST=192.168.100.40 LEN=182 TOS=0x00 PREC=0x00 TTL=255 ID=60919 PROTO=UDP SPT=42913 DPT=49691 LEN=162 
[  +0.000654] [UFW BLOCK] IN=ens18 OUT= MAC=02:aa:01:ee:fa:ec:02:aa:01:ea:46:06:08:00 SRC=192.168.100.28 DST=192.168.100.40 LEN=56 TOS=0x00 PREC=0x00 TTL=255 ID=60925 PROTO=UDP SPT=50261 DPT=26347 LEN=36 
[Nov18 15:15] [UFW BLOCK] IN=ens18 OUT= MAC=02:aa:01:ee:fa:ec:02:aa:01:b5:df:2c:08:00 SRC=192.168.100.27 DST=192.168.100.40 LEN=199 TOS=0x00 PREC=0x00 TTL=255 ID=32669 PROTO=UDP SPT=58375 DPT=9993 LEN=179 
[  +0.000009] [UFW BLOCK] IN=ens18 OUT= MAC=02:aa:01:ee:fa:ec:02:aa:01:b5:df:2c:08:00 SRC=192.168.100.27 DST=192.168.100.40 LEN=199 TOS=0x00 PREC=0x00 TTL=255 ID=32670 PROTO=UDP SPT=9993 DPT=9993 LEN=179 
[  +0.000005] [UFW BLOCK] IN=ens18 OUT= MAC=02:aa:01:ee:fa:ec:02:aa:01:b5:df:2c:08:00 SRC=192.168.100.27 DST=192.168.100.40 LEN=199 TOS=0x00 PREC=0x00 TTL=255 ID=32671 PROTO=UDP SPT=52197 DPT=9993 LEN=179 
[  +0.000011] [UFW BLOCK] IN=ens18 OUT= MAC=02:aa:01:ee:fa:ec:02:aa:01:b5:df:2c:08:00 SRC=192.168.100.27 DST=192.168.100.40 LEN=199 TOS=0x00 PREC=0x00 TTL=255 ID=32672 PROTO=UDP SPT=58375 DPT=9993 LEN=179 
[  +0.000003] [UFW BLOCK] IN=ens18 OUT= MAC=02:aa:01:ee:fa:ec:02:aa:01:b5:df:2c:08:00 SRC=192.168.100.27 DST=192.168.100.40 LEN=199 TOS=0x00 PREC=0x00 TTL=255 ID=32673 PROTO=UDP SPT=9993 DPT=9993 LEN=179 
[  +0.000004] [UFW BLOCK] IN=ens18 OUT= MAC=02:aa:01:ee:fa:ec:02:aa:01:b5:df:2c:08:00 SRC=192.168.100.27 DST=192.168.100.40 LEN=199 TOS=0x00 PREC=0x00 TTL=255 ID=32674 PROTO=UDP SPT=52197 DPT=9993 LEN=179 
[  +0.000004] [UFW BLOCK] IN=ens18 OUT= MAC=02:aa:01:ee:fa:ec:02:aa:01:b5:df:2c:08:00 SRC=192.168.100.27 DST=192.168.100.40 LEN=199 TOS=0x00 PREC=0x00 TTL=255 ID=32675 PROTO=UDP SPT=58375 DPT=9993 LEN=179 
[ +24.194084] [UFW BLOCK] IN=ens18 OUT= MAC=02:aa:01:ee:fa:ec:02:aa:01:ea:46:06:08:00 SRC=192.168.100.28 DST=192.168.100.40 LEN=182 TOS=0x00 PREC=0x00 TTL=255 ID=1105 PROTO=UDP SPT=42913 DPT=49691 LEN=162 
[  +0.000013] [UFW BLOCK] IN=ens18 OUT= MAC=02:aa:01:ee:fa:ec:02:aa:01:ea:46:06:08:00 SRC=192.168.100.28 DST=192.168.100.40 LEN=182 TOS=0x00 PREC=0x00 TTL=255 ID=1108 PROTO=UDP SPT=42913 DPT=49691 LEN=162 
[Nov18 15:16] [UFW BLOCK] IN=ens18 OUT= MAC=02:aa:01:ee:fa:ec:02:aa:01:ea:46:06:08:00 SRC=192.168.100.28 DST=192.168.100.40 LEN=182 TOS=0x00 PREC=0x00 TTL=255 ID=9380 PROTO=UDP SPT=42913 DPT=49691 LEN=162 
[  +0.000032] [UFW BLOCK] IN=ens18 OUT= MAC=02:aa:01:ee:fa:ec:02:aa:01:ea:46:06:08:00 SRC=192.168.100.28 DST=192.168.100.40 LEN=182 TOS=0x00 PREC=0x00 TTL=255 ID=9383 PROTO=UDP SPT=42913 DPT=49691 LEN=162 
[  +0.000801] [UFW BLOCK] IN=ens18 OUT= MAC=02:aa:01:ee:fa:ec:02:aa:01:ea:46:06:08:00 SRC=192.168.100.28 DST=192.168.100.40 LEN=56 TOS=0x00 PREC=0x00 TTL=255 ID=9389 PROTO=UDP SPT=50261 DPT=26347 LEN=36 
[Nov18 15:17] [UFW BLOCK] IN=ens18 OUT= MAC=02:aa:01:ee:fa:ec:02:aa:01:b5:df:2c:08:00 SRC=192.168.100.27 DST=192.168.100.40 LEN=56 TOS=0x00 PREC=0x00 TTL=255 ID=44860 PROTO=UDP SPT=9993 DPT=49691 LEN=36 
[ +22.523328] [UFW BLOCK] IN=ens18 OUT= MAC=02:aa:01:ee:fa:ec:02:aa:01:ea:46:06:08:00 SRC=192.168.100.28 DST=192.168.100.40 LEN=182 TOS=0x00 PREC=0x00 TTL=255 ID=16611 PROTO=UDP SPT=42913 DPT=49691 LEN=162 
[  +0.000053] [UFW BLOCK] IN=ens18 OUT= MAC=02:aa:01:ee:fa:ec:02:aa:01:ea:46:06:08:00 SRC=192.168.100.28 DST=192.168.100.40 LEN=182 TOS=0x00 PREC=0x00 TTL=255 ID=16614 PROTO=UDP SPT=42913 DPT=49691 LEN=162 
[Nov18 15:18] [UFW BLOCK] IN=ens18 OUT= MAC=02:aa:01:ee:fa:ec:02:aa:01:ea:46:06:08:00 SRC=192.168.100.28 DST=192.168.100.40 LEN=182 TOS=0x00 PREC=0x00 TTL=255 ID=24867 PROTO=UDP SPT=42913 DPT=49691 LEN=162 
[  +0.000025] [UFW BLOCK] IN=ens18 OUT= MAC=02:aa:01:ee:fa:ec:02:aa:01:ea:46:06:08:00 SRC=192.168.100.28 DST=192.168.100.40 LEN=182 TOS=0x00 PREC=0x00 TTL=255 ID=24870 PROTO=UDP SPT=42913 DPT=49691 LEN=162 
[  +0.000583] [UFW BLOCK] IN=ens18 OUT= MAC=02:aa:01:ee:fa:ec:02:aa:01:ea:46:06:08:00 SRC=192.168.100.28 DST=192.168.100.40 LEN=56 TOS=0x00 PREC=0x00 TTL=255 ID=24876 PROTO=UDP SPT=50261 DPT=26347 LEN=36 
[Nov18 15:19] [UFW BLOCK] IN=ens18 OUT= MAC=02:aa:01:ee:fa:ec:02:aa:01:b5:df:2c:08:00 SRC=192.168.100.27 DST=192.168.100.40 LEN=56 TOS=0x00 PREC=0x00 TTL=255 ID=62797 PROTO=UDP SPT=9993 DPT=49691 LEN=36 
[ +22.452051] [UFW BLOCK] IN=ens18 OUT= MAC=02:aa:01:ee:fa:ec:02:aa:01:ea:46:06:08:00 SRC=192.168.100.28 DST=192.168.100.40 LEN=182 TOS=0x00 PREC=0x00 TTL=255 ID=31831 PROTO=UDP SPT=42913 DPT=49691 LEN=162 
[  +0.000017] [UFW BLOCK] IN=ens18 OUT= MAC=02:aa:01:ee:fa:ec:02:aa:01:ea:46:06:08:00 SRC=192.168.100.28 DST=192.168.100.40 LEN=182 TOS=0x00 PREC=0x00 TTL=255 ID=31834 PROTO=UDP SPT=42913 DPT=49691 LEN=162 
[Nov18 15:20] [UFW BLOCK] IN=ens18 OUT= MAC=02:aa:01:ee:fa:ec:02:aa:01:ea:46:06:08:00 SRC=192.168.100.28 DST=192.168.100.40 LEN=182 TOS=0x00 PREC=0x00 TTL=255 ID=41099 PROTO=UDP SPT=42913 DPT=49691 LEN=162 
[  +0.000099] [UFW BLOCK] IN=ens18 OUT= MAC=02:aa:01:ee:fa:ec:02:aa:01:ea:46:06:08:00 SRC=192.168.100.28 DST=192.168.100.40 LEN=182 TOS=0x00 PREC=0x00 TTL=255 ID=41102 PROTO=UDP SPT=42913 DPT=49691 LEN=162 
[  +0.000632] [UFW BLOCK] IN=ens18 OUT= MAC=02:aa:01:ee:fa:ec:02:aa:01:ea:46:06:08:00 SRC=192.168.100.28 DST=192.168.100.40 LEN=56 TOS=0x00 PREC=0x00 TTL=255 ID=41108 PROTO=UDP SPT=50261 DPT=26347 LEN=36 
[Nov18 15:21] [UFW BLOCK] IN=ens18 OUT= MAC=02:aa:01:ee:fa:ec:02:aa:01:72:17:bf:08:00 SRC=192.168.100.19 DST=192.168.100.40 LEN=199 TOS=0x00 PREC=0x00 TTL=255 ID=40783 PROTO=UDP SPT=29975 DPT=9993 LEN=179 
[ +35.694013] [UFW BLOCK] IN=ens18 OUT= MAC=02:aa:01:ee:fa:ec:02:aa:01:ea:46:06:08:00 SRC=192.168.100.28 DST=192.168.100.40 LEN=182 TOS=0x00 PREC=0x00 TTL=255 ID=46987 PROTO=UDP SPT=42913 DPT=49691 LEN=162 
[  +0.000048] [UFW BLOCK] IN=ens18 OUT= MAC=02:aa:01:ee:fa:ec:02:aa:01:ea:46:06:08:00 SRC=192.168.100.28 DST=192.168.100.40 LEN=182 TOS=0x00 PREC=0x00 TTL=255 ID=46990 PROTO=UDP SPT=42913 DPT=49691 LEN=162 
[Nov18 15:22] [UFW BLOCK] IN=ens18 OUT= MAC=02:aa:01:ee:fa:ec:02:aa:01:72:17:bf:08:00 SRC=192.168.100.19 DST=192.168.100.40 LEN=199 TOS=0x00 PREC=0x00 TTL=255 ID=49835 PROTO=UDP SPT=29975 DPT=9993 LEN=179 
[ +34.641784] [UFW BLOCK] IN=ens18 OUT= MAC=02:aa:01:ee:fa:ec:02:aa:01:ea:46:06:08:00 SRC=192.168.100.28 DST=192.168.100.40 LEN=182 TOS=0x00 PREC=0x00 TTL=255 ID=55225 PROTO=UDP SPT=42913 DPT=49691 LEN=162 
[  +0.000021] [UFW BLOCK] IN=ens18 OUT= MAC=02:aa:01:ee:fa:ec:02:aa:01:ea:46:06:08:00 SRC=192.168.100.28 DST=192.168.100.40 LEN=182 TOS=0x00 PREC=0x00 TTL=255 ID=55228 PROTO=UDP SPT=42913 DPT=49691 LEN=162 
[Nov18 15:23] [UFW BLOCK] IN=ens18 OUT= MAC=02:aa:01:ee:fa:ec:02:aa:01:72:17:bf:08:00 SRC=192.168.100.19 DST=192.168.100.40 LEN=199 TOS=0x00 PREC=0x00 TTL=255 ID=61600 PROTO=UDP SPT=29975 DPT=9993 LEN=179 
[ +34.590747] [UFW BLOCK] IN=ens18 OUT= MAC=02:aa:01:ee:fa:ec:02:aa:01:ea:46:06:08:00 SRC=192.168.100.28 DST=192.168.100.40 LEN=182 TOS=0x00 PREC=0x00 TTL=255 ID=63440 PROTO=UDP SPT=42913 DPT=49691 LEN=162 
[  +0.000016] [UFW BLOCK] IN=ens18 OUT= MAC=02:aa:01:ee:fa:ec:02:aa:01:ea:46:06:08:00 SRC=192.168.100.28 DST=192.168.100.40 LEN=182 TOS=0x00 PREC=0x00 TTL=255 ID=63443 PROTO=UDP SPT=42913 DPT=49691 LEN=162 
[Nov18 15:24] [UFW BLOCK] IN=ens18 OUT= MAC=02:aa:01:ee:fa:ec:02:aa:01:72:17:bf:08:00 SRC=192.168.100.19 DST=192.168.100.40 LEN=199 TOS=0x00 PREC=0x00 TTL=255 ID=5177 PROTO=UDP SPT=29975 DPT=9993 LEN=179 
[ +34.535984] [UFW BLOCK] IN=ens18 OUT= MAC=02:aa:01:ee:fa:ec:02:aa:01:ea:46:06:08:00 SRC=192.168.100.28 DST=192.168.100.40 LEN=182 TOS=0x00 PREC=0x00 TTL=255 ID=3874 PROTO=UDP SPT=42913 DPT=49691 LEN=162 
[  +0.000012] [UFW BLOCK] IN=ens18 OUT= MAC=02:aa:01:ee:fa:ec:02:aa:01:ea:46:06:08:00 SRC=192.168.100.28 DST=192.168.100.40 LEN=182 TOS=0x00 PREC=0x00 TTL=255 ID=3877 PROTO=UDP SPT=42913 DPT=49691 LEN=162 
[Nov18 15:25] [UFW BLOCK] IN=ens18 OUT= MAC=02:aa:01:ee:fa:ec:02:aa:01:72:17:bf:08:00 SRC=192.168.100.19 DST=192.168.100.40 LEN=199 TOS=0x00 PREC=0x00 TTL=255 ID=12383 PROTO=UDP SPT=29975 DPT=9993 LEN=179 
[ +34.482728] [UFW BLOCK] IN=ens18 OUT= MAC=02:aa:01:ee:fa:ec:02:aa:01:ea:46:06:08:00 SRC=192.168.100.28 DST=192.168.100.40 LEN=182 TOS=0x00 PREC=0x00 TTL=255 ID=12463 PROTO=UDP SPT=42913 DPT=49691 LEN=162 
[  +0.000019] [UFW BLOCK] IN=ens18 OUT= MAC=02:aa:01:ee:fa:ec:02:aa:01:ea:46:06:08:00 SRC=192.168.100.28 DST=192.168.100.40 LEN=182 TOS=0x00 PREC=0x00 TTL=255 ID=12466 PROTO=UDP SPT=42913 DPT=49691 LEN=162 
[Nov18 15:26] [UFW BLOCK] IN=ens18 OUT= MAC=02:aa:01:ee:fa:ec:02:aa:01:72:17:bf:08:00 SRC=192.168.100.19 DST=192.168.100.40 LEN=199 TOS=0x00 PREC=0x00 TTL=255 ID=20446 PROTO=UDP SPT=29975 DPT=9993 LEN=179 
[ +34.428205] [UFW BLOCK] IN=ens18 OUT= MAC=02:aa:01:ee:fa:ec:02:aa:01:ea:46:06:08:00 SRC=192.168.100.28 DST=192.168.100.40 LEN=182 TOS=0x00 PREC=0x00 TTL=255 ID=19732 PROTO=UDP SPT=42913 DPT=49691 LEN=162 
[  +0.000055] [UFW BLOCK] IN=ens18 OUT= MAC=02:aa:01:ee:fa:ec:02:aa:01:ea:46:06:08:00 SRC=192.168.100.28 DST=192.168.100.40 LEN=182 TOS=0x00 PREC=0x00 TTL=255 ID=19735 PROTO=UDP SPT=42913 DPT=49691 LEN=162 
[Nov18 15:27] [UFW BLOCK] IN=ens18 OUT= MAC=02:aa:01:ee:fa:ec:02:aa:01:72:17:bf:08:00 SRC=192.168.100.19 DST=192.168.100.40 LEN=199 TOS=0x00 PREC=0x00 TTL=255 ID=24390 PROTO=UDP SPT=35151 DPT=9993 LEN=179 
[ +34.376444] [UFW BLOCK] IN=ens18 OUT= MAC=02:aa:01:ee:fa:ec:02:aa:01:ea:46:06:08:00 SRC=192.168.100.28 DST=192.168.100.40 LEN=182 TOS=0x00 PREC=0x00 TTL=255 ID=29674 PROTO=UDP SPT=42913 DPT=49691 LEN=162 
[  +0.000069] [UFW BLOCK] IN=ens18 OUT= MAC=02:aa:01:ee:fa:ec:02:aa:01:ea:46:06:08:00 SRC=192.168.100.28 DST=192.168.100.40 LEN=182 TOS=0x00 PREC=0x00 TTL=255 ID=29677 PROTO=UDP SPT=42913 DPT=49691 LEN=162 
[Nov18 15:28] [UFW BLOCK] IN=ens18 OUT= MAC=02:aa:01:ee:fa:ec:02:aa:01:72:17:bf:08:00 SRC=192.168.100.19 DST=192.168.100.40 LEN=199 TOS=0x00 PREC=0x00 TTL=255 ID=30617 PROTO=UDP SPT=35151 DPT=9993 LEN=179 
[ +34.330433] [UFW BLOCK] IN=ens18 OUT= MAC=02:aa:01:ee:fa:ec:02:aa:01:ea:46:06:08:00 SRC=192.168.100.28 DST=192.168.100.40 LEN=182 TOS=0x00 PREC=0x00 TTL=255 ID=37761 PROTO=UDP SPT=42913 DPT=49691 LEN=162 
[  +0.000058] [UFW BLOCK] IN=ens18 OUT= MAC=02:aa:01:ee:fa:ec:02:aa:01:ea:46:06:08:00 SRC=192.168.100.28 DST=192.168.100.40 LEN=182 TOS=0x00 PREC=0x00 TTL=255 ID=37764 PROTO=UDP SPT=42913 DPT=49691 LEN=162 
[Nov18 15:29] [UFW BLOCK] IN=ens18 OUT= MAC=02:aa:01:ee:fa:ec:02:aa:01:72:17:bf:08:00 SRC=192.168.100.19 DST=192.168.100.40 LEN=199 TOS=0x00 PREC=0x00 TTL=255 ID=33383 PROTO=UDP SPT=35151 DPT=9993 LEN=179 
[ +34.282475] [UFW BLOCK] IN=ens18 OUT= MAC=02:aa:01:ee:fa:ec:02:aa:01:ea:46:06:08:00 SRC=192.168.100.28 DST=192.168.100.40 LEN=182 TOS=0x00 PREC=0x00 TTL=255 ID=46438 PROTO=UDP SPT=42913 DPT=49691 LEN=162 
[  +0.000078] [UFW BLOCK] IN=ens18 OUT= MAC=02:aa:01:ee:fa:ec:02:aa:01:ea:46:06:08:00 SRC=192.168.100.28 DST=192.168.100.40 LEN=182 TOS=0x00 PREC=0x00 TTL=255 ID=46441 PROTO=UDP SPT=42913 DPT=49691 LEN=162 
[Nov18 15:30] [UFW BLOCK] IN=ens18 OUT= MAC=02:aa:01:ee:fa:ec:02:aa:01:72:17:bf:08:00 SRC=192.168.100.19 DST=192.168.100.40 LEN=199 TOS=0x00 PREC=0x00 TTL=255 ID=42965 PROTO=UDP SPT=35151 DPT=9993 LEN=179 
[ +34.235368] [UFW BLOCK] IN=ens18 OUT= MAC=02:aa:01:ee:fa:ec:02:aa:01:ea:46:06:08:00 SRC=192.168.100.28 DST=192.168.100.40 LEN=182 TOS=0x00 PREC=0x00 TTL=255 ID=54566 PROTO=UDP SPT=42913 DPT=49691 LEN=162 
[  +0.000048] [UFW BLOCK] IN=ens18 OUT= MAC=02:aa:01:ee:fa:ec:02:aa:01:ea:46:06:08:00 SRC=192.168.100.28 DST=192.168.100.40 LEN=182 TOS=0x00 PREC=0x00 TTL=255 ID=54569 PROTO=UDP SPT=42913 DPT=49691 LEN=162 
[Nov18 15:31] [UFW BLOCK] IN=ens18 OUT= MAC=02:aa:01:ee:fa:ec:02:aa:01:72:17:bf:08:00 SRC=192.168.100.19 DST=192.168.100.40 LEN=199 TOS=0x00 PREC=0x00 TTL=255 ID=50154 PROTO=UDP SPT=35151 DPT=9993 LEN=179 
[ +34.187905] [UFW BLOCK] IN=ens18 OUT= MAC=02:aa:01:ee:fa:ec:02:aa:01:ea:46:06:08:00 SRC=192.168.100.28 DST=192.168.100.40 LEN=182 TOS=0x00 PREC=0x00 TTL=255 ID=63491 PROTO=UDP SPT=42913 DPT=49691 LEN=162 
[  +0.000026] [UFW BLOCK] IN=ens18 OUT= MAC=02:aa:01:ee:fa:ec:02:aa:01:ea:46:06:08:00 SRC=192.168.100.28 DST=192.168.100.40 LEN=182 TOS=0x00 PREC=0x00 TTL=255 ID=63494 PROTO=UDP SPT=42913 DPT=49691 LEN=162 
[Nov18 15:32] [UFW BLOCK] IN=ens18 OUT= MAC=02:aa:01:ee:fa:ec:02:aa:01:72:17:bf:08:00 SRC=192.168.100.19 DST=192.168.100.40 LEN=199 TOS=0x00 PREC=0x00 TTL=255 ID=60702 PROTO=UDP SPT=35151 DPT=9993 LEN=179 


==> etcd [b8bb42f78de9] <==
{"level":"warn","ts":"2025-11-18T15:01:15.811960Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:42750","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-18T15:01:15.818798Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:42790","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-18T15:01:15.824955Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:42794","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-18T15:01:15.830543Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:42818","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-18T15:01:15.838024Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:42844","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-18T15:01:15.843744Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:42868","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-18T15:01:15.868941Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:42876","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-18T15:01:15.874949Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:42902","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-18T15:01:15.880239Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:42922","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-18T15:01:15.887547Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:42932","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-18T15:01:15.894223Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:42944","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-18T15:01:15.899708Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:42956","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-18T15:01:15.905538Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:42982","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-18T15:01:15.910661Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:42996","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-18T15:01:15.921491Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:43020","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-18T15:01:15.926868Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:43042","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-18T15:01:15.932350Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:43064","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-18T15:01:15.937745Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:43080","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-18T15:01:15.943076Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:43092","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-18T15:01:15.949090Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:43106","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-18T15:01:15.956625Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:43120","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-18T15:01:15.962178Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:43142","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-18T15:01:15.970301Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:43168","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-18T15:01:15.977392Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:43184","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-18T15:01:15.983192Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:43190","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-18T15:01:15.988755Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:43202","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-18T15:01:15.993981Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:43220","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-18T15:01:15.999238Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:43228","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-18T15:01:16.004224Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:43258","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-18T15:01:16.009687Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:43264","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-18T15:01:16.025502Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:43278","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-18T15:01:16.030754Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:43294","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-18T15:01:16.035705Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:43306","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-18T15:01:16.040861Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:43330","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-18T15:01:16.045822Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:43356","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-18T15:01:16.051187Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:43380","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-18T15:01:16.056583Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:43398","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-18T15:01:16.061969Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:43420","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-18T15:01:16.067541Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:43438","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-18T15:01:16.072625Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:43456","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-18T15:01:16.077954Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:43470","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-18T15:01:16.083207Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:43486","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-18T15:01:16.121645Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:43508","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-18T15:01:16.126937Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:43530","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-18T15:01:16.173015Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:43558","server-name":"","error":"EOF"}
{"level":"info","ts":"2025-11-18T15:11:15.734551Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":652}
{"level":"info","ts":"2025-11-18T15:11:15.740889Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":652,"took":"5.920331ms","hash":1967719799,"current-db-size-bytes":1429504,"current-db-size":"1.4 MB","current-db-size-in-use-bytes":1429504,"current-db-size-in-use":"1.4 MB"}
{"level":"info","ts":"2025-11-18T15:11:15.740961Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":1967719799,"revision":652,"compact-revision":-1}
{"level":"info","ts":"2025-11-18T15:16:15.740847Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":892}
{"level":"info","ts":"2025-11-18T15:16:15.743237Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":892,"took":"2.111367ms","hash":1646943059,"current-db-size-bytes":1429504,"current-db-size":"1.4 MB","current-db-size-in-use-bytes":925696,"current-db-size-in-use":"926 kB"}
{"level":"info","ts":"2025-11-18T15:16:15.743312Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":1646943059,"revision":892,"compact-revision":652}
{"level":"info","ts":"2025-11-18T15:21:15.747449Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":1132}
{"level":"info","ts":"2025-11-18T15:21:15.750501Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":1132,"took":"2.615755ms","hash":869968979,"current-db-size-bytes":1429504,"current-db-size":"1.4 MB","current-db-size-in-use-bytes":999424,"current-db-size-in-use":"999 kB"}
{"level":"info","ts":"2025-11-18T15:21:15.750566Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":869968979,"revision":1132,"compact-revision":892}
{"level":"info","ts":"2025-11-18T15:26:15.753625Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":1400}
{"level":"info","ts":"2025-11-18T15:26:15.757029Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":1400,"took":"2.820935ms","hash":398679813,"current-db-size-bytes":1429504,"current-db-size":"1.4 MB","current-db-size-in-use-bytes":1171456,"current-db-size-in-use":"1.2 MB"}
{"level":"info","ts":"2025-11-18T15:26:15.757072Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":398679813,"revision":1400,"compact-revision":1132}
{"level":"info","ts":"2025-11-18T15:31:15.760757Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":1711}
{"level":"info","ts":"2025-11-18T15:31:15.763687Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":1711,"took":"2.499157ms","hash":4085459833,"current-db-size-bytes":1429504,"current-db-size":"1.4 MB","current-db-size-in-use-bytes":1187840,"current-db-size-in-use":"1.2 MB"}
{"level":"info","ts":"2025-11-18T15:31:15.763765Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":4085459833,"revision":1711,"compact-revision":1400}


==> kernel <==
 15:32:34 up 5 days, 22:22,  0 users,  load average: 0.22, 0.19, 0.18
Linux minikube 6.12.48+deb13-amd64 #1 SMP PREEMPT_DYNAMIC Debian 6.12.48-1 (2025-09-20) x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.5 LTS"


==> kube-apiserver [8ad509116fdf] <==
I1118 15:01:18.988299       1 alloc.go:328] "allocated clusterIPs" service="kube-system/kube-dns" clusterIPs={"IPv4":"10.96.0.10"}
I1118 15:01:18.994250       1 controller.go:667] quota admission added evaluator for: daemonsets.apps
I1118 15:01:24.197091       1 controller.go:667] quota admission added evaluator for: controllerrevisions.apps
I1118 15:01:24.399484       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I1118 15:01:24.402528       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I1118 15:01:24.598598       1 controller.go:667] quota admission added evaluator for: replicasets.apps
I1118 15:02:23.871567       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1118 15:02:35.200779       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1118 15:03:27.527711       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1118 15:03:55.615039       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1118 15:04:28.704239       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1118 15:05:10.582873       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1118 15:05:39.606184       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1118 15:06:25.207354       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1118 15:06:57.959068       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1118 15:07:35.812114       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1118 15:08:03.942247       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1118 15:08:50.815219       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1118 15:09:06.993216       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1118 15:10:02.835290       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1118 15:10:25.272206       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1118 15:11:09.782779       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1118 15:11:16.503839       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I1118 15:11:54.062395       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1118 15:12:30.564528       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1118 15:13:20.223000       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1118 15:13:35.433867       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1118 15:14:38.185629       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1118 15:14:39.733939       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1118 15:15:51.656159       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1118 15:16:04.527943       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1118 15:16:55.226514       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1118 15:17:28.044811       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1118 15:18:14.294322       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1118 15:18:32.332115       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1118 15:19:43.800877       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1118 15:19:56.529121       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1118 15:21:05.322764       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1118 15:21:16.504556       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I1118 15:21:26.467290       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1118 15:22:13.517828       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1118 15:22:32.570777       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1118 15:23:24.920319       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1118 15:23:51.664783       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1118 15:24:26.336325       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1118 15:25:00.185404       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1118 15:25:38.267745       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1118 15:26:11.722033       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1118 15:26:55.403295       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1118 15:27:25.117654       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1118 15:28:22.833838       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1118 15:28:28.527077       1 alloc.go:328] "allocated clusterIPs" service="default/fastapi-service" clusterIPs={"IPv4":"10.105.73.145"}
I1118 15:28:46.712050       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1118 15:29:28.968987       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1118 15:30:00.000231       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1118 15:30:55.498556       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1118 15:31:02.853217       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1118 15:31:16.505768       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I1118 15:32:00.341153       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1118 15:32:11.270820       1 stats.go:136] "Error getting keys" err="empty key: \"\""


==> kube-controller-manager [4cdaa8a22af7] <==
I1118 15:01:23.246066       1 shared_informer.go:349] "Waiting for caches to sync" controller="ReplicationController"
I1118 15:01:23.398279       1 controllermanager.go:781] "Started controller" controller="job-controller"
I1118 15:01:23.398400       1 job_controller.go:257] "Starting job controller" logger="job-controller"
I1118 15:01:23.398417       1 shared_informer.go:349] "Waiting for caches to sync" controller="job"
I1118 15:01:23.404330       1 shared_informer.go:349] "Waiting for caches to sync" controller="resource quota"
I1118 15:01:23.416076       1 actual_state_of_world.go:541] "Failed to update statusUpdateNeeded field in actual state of world" logger="persistentvolume-attach-detach-controller" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"minikube\" does not exist"
I1118 15:01:23.416123       1 shared_informer.go:349] "Waiting for caches to sync" controller="garbage collector"
I1118 15:01:23.422852       1 shared_informer.go:356] "Caches are synced" controller="validatingadmissionpolicy-status"
I1118 15:01:23.428310       1 shared_informer.go:356] "Caches are synced" controller="endpoint_slice"
I1118 15:01:23.437155       1 shared_informer.go:356] "Caches are synced" controller="HPA"
I1118 15:01:23.445023       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrapproving"
I1118 15:01:23.445036       1 shared_informer.go:356] "Caches are synced" controller="taint"
I1118 15:01:23.445104       1 node_lifecycle_controller.go:1221] "Initializing eviction metric for zone" logger="node-lifecycle-controller" zone=""
I1118 15:01:23.445195       1 node_lifecycle_controller.go:873] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube"
I1118 15:01:23.445277       1 node_lifecycle_controller.go:1067] "Controller detected that zone is now in new state" logger="node-lifecycle-controller" zone="" newState="Normal"
I1118 15:01:23.446219       1 shared_informer.go:356] "Caches are synced" controller="TTL after finished"
I1118 15:01:23.446244       1 shared_informer.go:356] "Caches are synced" controller="ReplicationController"
I1118 15:01:23.446866       1 shared_informer.go:356] "Caches are synced" controller="PV protection"
I1118 15:01:23.446908       1 shared_informer.go:356] "Caches are synced" controller="GC"
I1118 15:01:23.446922       1 shared_informer.go:356] "Caches are synced" controller="cronjob"
I1118 15:01:23.446938       1 shared_informer.go:356] "Caches are synced" controller="daemon sets"
I1118 15:01:23.446925       1 shared_informer.go:356] "Caches are synced" controller="PVC protection"
I1118 15:01:23.446952       1 shared_informer.go:356] "Caches are synced" controller="taint-eviction-controller"
I1118 15:01:23.447015       1 shared_informer.go:356] "Caches are synced" controller="legacy-service-account-token-cleaner"
I1118 15:01:23.448228       1 shared_informer.go:356] "Caches are synced" controller="endpoint_slice_mirroring"
I1118 15:01:23.448236       1 shared_informer.go:356] "Caches are synced" controller="persistent volume"
I1118 15:01:23.451545       1 shared_informer.go:356] "Caches are synced" controller="resource quota"
I1118 15:01:23.451574       1 shared_informer.go:356] "Caches are synced" controller="node"
I1118 15:01:23.451658       1 range_allocator.go:177] "Sending events to api server" logger="node-ipam-controller"
I1118 15:01:23.451704       1 range_allocator.go:183] "Starting range CIDR allocator" logger="node-ipam-controller"
I1118 15:01:23.451729       1 shared_informer.go:349] "Waiting for caches to sync" controller="cidrallocator"
I1118 15:01:23.451749       1 shared_informer.go:356] "Caches are synced" controller="cidrallocator"
I1118 15:01:23.456732       1 shared_informer.go:356] "Caches are synced" controller="ephemeral"
I1118 15:01:23.460990       1 range_allocator.go:428] "Set node PodCIDR" logger="node-ipam-controller" node="minikube" podCIDRs=["10.244.0.0/24"]
I1118 15:01:23.494329       1 shared_informer.go:356] "Caches are synced" controller="garbage collector"
I1118 15:01:23.494351       1 garbagecollector.go:154] "Garbage collector: all resource monitors have synced" logger="garbage-collector-controller"
I1118 15:01:23.494361       1 garbagecollector.go:157] "Proceeding to collect garbage" logger="garbage-collector-controller"
I1118 15:01:23.495367       1 shared_informer.go:356] "Caches are synced" controller="disruption"
I1118 15:01:23.495467       1 shared_informer.go:356] "Caches are synced" controller="crt configmap"
I1118 15:01:23.496529       1 shared_informer.go:356] "Caches are synced" controller="VAC protection"
I1118 15:01:23.496544       1 shared_informer.go:356] "Caches are synced" controller="service account"
I1118 15:01:23.496548       1 shared_informer.go:356] "Caches are synced" controller="expand"
I1118 15:01:23.496532       1 shared_informer.go:356] "Caches are synced" controller="bootstrap_signer"
I1118 15:01:23.496878       1 shared_informer.go:356] "Caches are synced" controller="ReplicaSet"
I1118 15:01:23.497671       1 shared_informer.go:356] "Caches are synced" controller="endpoint"
I1118 15:01:23.497680       1 shared_informer.go:356] "Caches are synced" controller="ClusterRoleAggregator"
I1118 15:01:23.497705       1 shared_informer.go:356] "Caches are synced" controller="service-cidr-controller"
I1118 15:01:23.497737       1 shared_informer.go:356] "Caches are synced" controller="TTL"
I1118 15:01:23.497882       1 shared_informer.go:356] "Caches are synced" controller="resource_claim"
I1118 15:01:23.498249       1 shared_informer.go:356] "Caches are synced" controller="deployment"
I1118 15:01:23.498531       1 shared_informer.go:356] "Caches are synced" controller="stateful set"
I1118 15:01:23.498533       1 shared_informer.go:356] "Caches are synced" controller="job"
I1118 15:01:23.498652       1 shared_informer.go:356] "Caches are synced" controller="namespace"
I1118 15:01:23.500393       1 shared_informer.go:356] "Caches are synced" controller="attach detach"
I1118 15:01:23.503748       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrsigning-kubelet-serving"
I1118 15:01:23.503950       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrsigning-kubelet-client"
I1118 15:01:23.504974       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrsigning-kube-apiserver-client"
I1118 15:01:23.505036       1 shared_informer.go:356] "Caches are synced" controller="resource quota"
I1118 15:01:23.505040       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrsigning-legacy-unknown"
I1118 15:01:23.516479       1 shared_informer.go:356] "Caches are synced" controller="garbage collector"


==> kube-proxy [5d921d66a074] <==
I1118 15:01:24.860286       1 server_linux.go:53] "Using iptables proxy"
I1118 15:01:25.023426       1 shared_informer.go:349] "Waiting for caches to sync" controller="node informer cache"
I1118 15:01:25.124347       1 shared_informer.go:356] "Caches are synced" controller="node informer cache"
I1118 15:01:25.124432       1 server.go:219] "Successfully retrieved NodeIPs" NodeIPs=["192.168.49.2"]
E1118 15:01:25.124549       1 server.go:256] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I1118 15:01:25.145414       1 server.go:265] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I1118 15:01:25.145455       1 server_linux.go:132] "Using iptables Proxier"
I1118 15:01:25.150186       1 proxier.go:242] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
I1118 15:01:25.157526       1 server.go:527] "Version info" version="v1.34.0"
I1118 15:01:25.157549       1 server.go:529] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1118 15:01:25.159138       1 config.go:106] "Starting endpoint slice config controller"
I1118 15:01:25.159158       1 shared_informer.go:349] "Waiting for caches to sync" controller="endpoint slice config"
I1118 15:01:25.159176       1 config.go:200] "Starting service config controller"
I1118 15:01:25.159181       1 shared_informer.go:349] "Waiting for caches to sync" controller="service config"
I1118 15:01:25.159333       1 config.go:403] "Starting serviceCIDR config controller"
I1118 15:01:25.159349       1 shared_informer.go:349] "Waiting for caches to sync" controller="serviceCIDR config"
I1118 15:01:25.159565       1 config.go:309] "Starting node config controller"
I1118 15:01:25.159578       1 shared_informer.go:349] "Waiting for caches to sync" controller="node config"
I1118 15:01:25.159585       1 shared_informer.go:356] "Caches are synced" controller="node config"
I1118 15:01:25.259271       1 shared_informer.go:356] "Caches are synced" controller="service config"
I1118 15:01:25.259283       1 shared_informer.go:356] "Caches are synced" controller="endpoint slice config"
I1118 15:01:25.259405       1 shared_informer.go:356] "Caches are synced" controller="serviceCIDR config"


==> kube-scheduler [a473f83685d6] <==
I1118 15:01:16.050825       1 serving.go:386] Generated self-signed cert in-memory
W1118 15:01:16.488304       1 requestheader_controller.go:204] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W1118 15:01:16.488329       1 authentication.go:397] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W1118 15:01:16.488338       1 authentication.go:398] Continuing without authentication configuration. This may treat all requests as anonymous.
W1118 15:01:16.488346       1 authentication.go:399] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I1118 15:01:16.517382       1 server.go:175] "Starting Kubernetes Scheduler" version="v1.34.0"
I1118 15:01:16.517406       1 server.go:177] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1118 15:01:16.519534       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I1118 15:01:16.519565       1 shared_informer.go:349] "Waiting for caches to sync" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I1118 15:01:16.519712       1 secure_serving.go:211] Serving securely on 127.0.0.1:10259
I1118 15:01:16.519761       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
E1118 15:01:16.520455       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError" reflector="runtime/asm_amd64.s:1700" type="*v1.ConfigMap"
E1118 15:01:16.521867       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ResourceClaim: resourceclaims.resource.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"resourceclaims\" in API group \"resource.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ResourceClaim"
E1118 15:01:16.522042       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Node"
E1118 15:01:16.522095       1 reflector.go:205] "Failed to watch" err="failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolumeClaim"
E1118 15:01:16.522123       1 reflector.go:205] "Failed to watch" err="failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StatefulSet"
E1118 15:01:16.522131       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Pod"
E1118 15:01:16.522177       1 reflector.go:205] "Failed to watch" err="failed to list *v1.DeviceClass: deviceclasses.resource.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"deviceclasses\" in API group \"resource.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.DeviceClass"
E1118 15:01:16.522184       1 reflector.go:205] "Failed to watch" err="failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PodDisruptionBudget"
E1118 15:01:16.522202       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ResourceSlice: resourceslices.resource.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"resourceslices\" in API group \"resource.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ResourceSlice"
E1118 15:01:16.522264       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Namespace"
E1118 15:01:16.522302       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicationController"
E1118 15:01:16.522339       1 reflector.go:205] "Failed to watch" err="failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSINode"
E1118 15:01:16.522352       1 reflector.go:205] "Failed to watch" err="failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StorageClass"
E1118 15:01:16.522395       1 reflector.go:205] "Failed to watch" err="failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolume"
E1118 15:01:16.522403       1 reflector.go:205] "Failed to watch" err="failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIDriver"
E1118 15:01:16.522433       1 reflector.go:205] "Failed to watch" err="failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIStorageCapacity"
E1118 15:01:16.522443       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Service"
E1118 15:01:16.522514       1 reflector.go:205] "Failed to watch" err="failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"volumeattachments\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.VolumeAttachment"
E1118 15:01:16.522557       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicaSet"
E1118 15:01:17.326049       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError" reflector="runtime/asm_amd64.s:1700" type="*v1.ConfigMap"
E1118 15:01:17.405542       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicaSet"
E1118 15:01:17.418609       1 reflector.go:205] "Failed to watch" err="failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolumeClaim"
E1118 15:01:17.426083       1 reflector.go:205] "Failed to watch" err="failed to list *v1.DeviceClass: deviceclasses.resource.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"deviceclasses\" in API group \"resource.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.DeviceClass"
E1118 15:01:17.446442       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ResourceSlice: resourceslices.resource.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"resourceslices\" in API group \"resource.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ResourceSlice"
E1118 15:01:17.587931       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ResourceClaim: resourceclaims.resource.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"resourceclaims\" in API group \"resource.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ResourceClaim"
E1118 15:01:17.594714       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Pod"
E1118 15:01:17.657833       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Namespace"
E1118 15:01:17.679593       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Service"
E1118 15:01:17.684399       1 reflector.go:205] "Failed to watch" err="failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"volumeattachments\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.VolumeAttachment"
E1118 15:01:17.708111       1 reflector.go:205] "Failed to watch" err="failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSINode"
E1118 15:01:17.760098       1 reflector.go:205] "Failed to watch" err="failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIDriver"
E1118 15:01:17.791948       1 reflector.go:205] "Failed to watch" err="failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StorageClass"
I1118 15:01:20.120340       1 shared_informer.go:356] "Caches are synced" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"


==> kubelet <==
Nov 18 15:20:39 minikube kubelet[2317]: E1118 15:20:39.255841    2317 kuberuntime_manager.go:1449] "Unhandled Error" err="container fastapi start failed in pod fastapi-pod_default(0531beeb-fa46-4794-b442-4a12e5727da4): ErrImagePull: Error response from daemon: pull access denied for fastapi-k8s-demo, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
Nov 18 15:20:39 minikube kubelet[2317]: E1118 15:20:39.255874    2317 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fastapi\" with ErrImagePull: \"Error response from daemon: pull access denied for fastapi-k8s-demo, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/fastapi-pod" podUID="0531beeb-fa46-4794-b442-4a12e5727da4"
Nov 18 15:20:51 minikube kubelet[2317]: E1118 15:20:51.906479    2317 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fastapi\" with ImagePullBackOff: \"Back-off pulling image \\\"fastapi-k8s-demo:latest\\\": ErrImagePull: Error response from daemon: pull access denied for fastapi-k8s-demo, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/fastapi-pod" podUID="0531beeb-fa46-4794-b442-4a12e5727da4"
Nov 18 15:21:05 minikube kubelet[2317]: E1118 15:21:05.907052    2317 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fastapi\" with ImagePullBackOff: \"Back-off pulling image \\\"fastapi-k8s-demo:latest\\\": ErrImagePull: Error response from daemon: pull access denied for fastapi-k8s-demo, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/fastapi-pod" podUID="0531beeb-fa46-4794-b442-4a12e5727da4"
Nov 18 15:21:17 minikube kubelet[2317]: E1118 15:21:17.906280    2317 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fastapi\" with ImagePullBackOff: \"Back-off pulling image \\\"fastapi-k8s-demo:latest\\\": ErrImagePull: Error response from daemon: pull access denied for fastapi-k8s-demo, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/fastapi-pod" podUID="0531beeb-fa46-4794-b442-4a12e5727da4"
Nov 18 15:21:31 minikube kubelet[2317]: E1118 15:21:31.246838    2317 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for fastapi-k8s-demo, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="fastapi-k8s-demo:latest"
Nov 18 15:21:31 minikube kubelet[2317]: E1118 15:21:31.246886    2317 kuberuntime_image.go:43] "Failed to pull image" err="Error response from daemon: pull access denied for fastapi-k8s-demo, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="fastapi-k8s-demo:latest"
Nov 18 15:21:31 minikube kubelet[2317]: E1118 15:21:31.246962    2317 kuberuntime_manager.go:1449] "Unhandled Error" err="container fastapi start failed in pod fastapi-pod_default(0531beeb-fa46-4794-b442-4a12e5727da4): ErrImagePull: Error response from daemon: pull access denied for fastapi-k8s-demo, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
Nov 18 15:21:31 minikube kubelet[2317]: E1118 15:21:31.246992    2317 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fastapi\" with ErrImagePull: \"Error response from daemon: pull access denied for fastapi-k8s-demo, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/fastapi-pod" podUID="0531beeb-fa46-4794-b442-4a12e5727da4"
Nov 18 15:21:44 minikube kubelet[2317]: E1118 15:21:44.906162    2317 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fastapi\" with ImagePullBackOff: \"Back-off pulling image \\\"fastapi-k8s-demo:latest\\\": ErrImagePull: Error response from daemon: pull access denied for fastapi-k8s-demo, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/fastapi-pod" podUID="0531beeb-fa46-4794-b442-4a12e5727da4"
Nov 18 15:21:59 minikube kubelet[2317]: E1118 15:21:59.906478    2317 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fastapi\" with ImagePullBackOff: \"Back-off pulling image \\\"fastapi-k8s-demo:latest\\\": ErrImagePull: Error response from daemon: pull access denied for fastapi-k8s-demo, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/fastapi-pod" podUID="0531beeb-fa46-4794-b442-4a12e5727da4"
Nov 18 15:22:10 minikube kubelet[2317]: E1118 15:22:10.906283    2317 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fastapi\" with ImagePullBackOff: \"Back-off pulling image \\\"fastapi-k8s-demo:latest\\\": ErrImagePull: Error response from daemon: pull access denied for fastapi-k8s-demo, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/fastapi-pod" podUID="0531beeb-fa46-4794-b442-4a12e5727da4"
Nov 18 15:22:22 minikube kubelet[2317]: E1118 15:22:22.906228    2317 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fastapi\" with ImagePullBackOff: \"Back-off pulling image \\\"fastapi-k8s-demo:latest\\\": ErrImagePull: Error response from daemon: pull access denied for fastapi-k8s-demo, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/fastapi-pod" podUID="0531beeb-fa46-4794-b442-4a12e5727da4"
Nov 18 15:22:33 minikube kubelet[2317]: E1118 15:22:33.906151    2317 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fastapi\" with ImagePullBackOff: \"Back-off pulling image \\\"fastapi-k8s-demo:latest\\\": ErrImagePull: Error response from daemon: pull access denied for fastapi-k8s-demo, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/fastapi-pod" podUID="0531beeb-fa46-4794-b442-4a12e5727da4"
Nov 18 15:22:45 minikube kubelet[2317]: E1118 15:22:45.906583    2317 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fastapi\" with ImagePullBackOff: \"Back-off pulling image \\\"fastapi-k8s-demo:latest\\\": ErrImagePull: Error response from daemon: pull access denied for fastapi-k8s-demo, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/fastapi-pod" podUID="0531beeb-fa46-4794-b442-4a12e5727da4"
Nov 18 15:23:02 minikube kubelet[2317]: E1118 15:23:02.261013    2317 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for fastapi-k8s-demo, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="fastapi-k8s-demo:latest"
Nov 18 15:23:02 minikube kubelet[2317]: E1118 15:23:02.261101    2317 kuberuntime_image.go:43] "Failed to pull image" err="Error response from daemon: pull access denied for fastapi-k8s-demo, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="fastapi-k8s-demo:latest"
Nov 18 15:23:02 minikube kubelet[2317]: E1118 15:23:02.261190    2317 kuberuntime_manager.go:1449] "Unhandled Error" err="container fastapi start failed in pod fastapi-pod_default(0531beeb-fa46-4794-b442-4a12e5727da4): ErrImagePull: Error response from daemon: pull access denied for fastapi-k8s-demo, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
Nov 18 15:23:02 minikube kubelet[2317]: E1118 15:23:02.261223    2317 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fastapi\" with ErrImagePull: \"Error response from daemon: pull access denied for fastapi-k8s-demo, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/fastapi-pod" podUID="0531beeb-fa46-4794-b442-4a12e5727da4"
Nov 18 15:23:14 minikube kubelet[2317]: E1118 15:23:14.906815    2317 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fastapi\" with ImagePullBackOff: \"Back-off pulling image \\\"fastapi-k8s-demo:latest\\\": ErrImagePull: Error response from daemon: pull access denied for fastapi-k8s-demo, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/fastapi-pod" podUID="0531beeb-fa46-4794-b442-4a12e5727da4"
Nov 18 15:23:25 minikube kubelet[2317]: E1118 15:23:25.907088    2317 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fastapi\" with ImagePullBackOff: \"Back-off pulling image \\\"fastapi-k8s-demo:latest\\\": ErrImagePull: Error response from daemon: pull access denied for fastapi-k8s-demo, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/fastapi-pod" podUID="0531beeb-fa46-4794-b442-4a12e5727da4"
Nov 18 15:23:39 minikube kubelet[2317]: E1118 15:23:39.907039    2317 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fastapi\" with ImagePullBackOff: \"Back-off pulling image \\\"fastapi-k8s-demo:latest\\\": ErrImagePull: Error response from daemon: pull access denied for fastapi-k8s-demo, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/fastapi-pod" podUID="0531beeb-fa46-4794-b442-4a12e5727da4"
Nov 18 15:23:50 minikube kubelet[2317]: E1118 15:23:50.907099    2317 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fastapi\" with ImagePullBackOff: \"Back-off pulling image \\\"fastapi-k8s-demo:latest\\\": ErrImagePull: Error response from daemon: pull access denied for fastapi-k8s-demo, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/fastapi-pod" podUID="0531beeb-fa46-4794-b442-4a12e5727da4"
Nov 18 15:24:03 minikube kubelet[2317]: E1118 15:24:03.906817    2317 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fastapi\" with ImagePullBackOff: \"Back-off pulling image \\\"fastapi-k8s-demo:latest\\\": ErrImagePull: Error response from daemon: pull access denied for fastapi-k8s-demo, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/fastapi-pod" podUID="0531beeb-fa46-4794-b442-4a12e5727da4"
Nov 18 15:24:17 minikube kubelet[2317]: E1118 15:24:17.906532    2317 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fastapi\" with ImagePullBackOff: \"Back-off pulling image \\\"fastapi-k8s-demo:latest\\\": ErrImagePull: Error response from daemon: pull access denied for fastapi-k8s-demo, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/fastapi-pod" podUID="0531beeb-fa46-4794-b442-4a12e5727da4"
Nov 18 15:24:31 minikube kubelet[2317]: E1118 15:24:31.906295    2317 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fastapi\" with ImagePullBackOff: \"Back-off pulling image \\\"fastapi-k8s-demo:latest\\\": ErrImagePull: Error response from daemon: pull access denied for fastapi-k8s-demo, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/fastapi-pod" podUID="0531beeb-fa46-4794-b442-4a12e5727da4"
Nov 18 15:24:42 minikube kubelet[2317]: E1118 15:24:42.906226    2317 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fastapi\" with ImagePullBackOff: \"Back-off pulling image \\\"fastapi-k8s-demo:latest\\\": ErrImagePull: Error response from daemon: pull access denied for fastapi-k8s-demo, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/fastapi-pod" podUID="0531beeb-fa46-4794-b442-4a12e5727da4"
Nov 18 15:24:54 minikube kubelet[2317]: E1118 15:24:54.907251    2317 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fastapi\" with ImagePullBackOff: \"Back-off pulling image \\\"fastapi-k8s-demo:latest\\\": ErrImagePull: Error response from daemon: pull access denied for fastapi-k8s-demo, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/fastapi-pod" podUID="0531beeb-fa46-4794-b442-4a12e5727da4"
Nov 18 15:25:00 minikube kubelet[2317]: I1118 15:25:00.615029    2317 reconciler_common.go:163] "operationExecutor.UnmountVolume started for volume \"kube-api-access-vjsv7\" (UniqueName: \"kubernetes.io/projected/0531beeb-fa46-4794-b442-4a12e5727da4-kube-api-access-vjsv7\") pod \"0531beeb-fa46-4794-b442-4a12e5727da4\" (UID: \"0531beeb-fa46-4794-b442-4a12e5727da4\") "
Nov 18 15:25:00 minikube kubelet[2317]: I1118 15:25:00.617257    2317 operation_generator.go:781] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/0531beeb-fa46-4794-b442-4a12e5727da4-kube-api-access-vjsv7" (OuterVolumeSpecName: "kube-api-access-vjsv7") pod "0531beeb-fa46-4794-b442-4a12e5727da4" (UID: "0531beeb-fa46-4794-b442-4a12e5727da4"). InnerVolumeSpecName "kube-api-access-vjsv7". PluginName "kubernetes.io/projected", VolumeGIDValue ""
Nov 18 15:25:00 minikube kubelet[2317]: I1118 15:25:00.715523    2317 reconciler_common.go:299] "Volume detached for volume \"kube-api-access-vjsv7\" (UniqueName: \"kubernetes.io/projected/0531beeb-fa46-4794-b442-4a12e5727da4-kube-api-access-vjsv7\") on node \"minikube\" DevicePath \"\""
Nov 18 15:25:00 minikube kubelet[2317]: I1118 15:25:00.911195    2317 kubelet_volumes.go:163] "Cleaned up orphaned pod volumes dir" podUID="0531beeb-fa46-4794-b442-4a12e5727da4" path="/var/lib/kubelet/pods/0531beeb-fa46-4794-b442-4a12e5727da4/volumes"
Nov 18 15:25:02 minikube kubelet[2317]: I1118 15:25:02.126274    2317 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-bff9l\" (UniqueName: \"kubernetes.io/projected/71c7a2ee-b30f-4f8a-8991-3305284c8ffb-kube-api-access-bff9l\") pod \"fastapi-pod\" (UID: \"71c7a2ee-b30f-4f8a-8991-3305284c8ffb\") " pod="default/fastapi-pod"
Nov 18 15:25:03 minikube kubelet[2317]: E1118 15:25:03.886230    2317 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for fastapi-k8s-demo, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="fastapi-k8s-demo:latest"
Nov 18 15:25:03 minikube kubelet[2317]: E1118 15:25:03.886289    2317 kuberuntime_image.go:43] "Failed to pull image" err="Error response from daemon: pull access denied for fastapi-k8s-demo, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="fastapi-k8s-demo:latest"
Nov 18 15:25:03 minikube kubelet[2317]: E1118 15:25:03.886369    2317 kuberuntime_manager.go:1449] "Unhandled Error" err="container fastapi start failed in pod fastapi-pod_default(71c7a2ee-b30f-4f8a-8991-3305284c8ffb): ErrImagePull: Error response from daemon: pull access denied for fastapi-k8s-demo, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
Nov 18 15:25:03 minikube kubelet[2317]: E1118 15:25:03.886399    2317 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fastapi\" with ErrImagePull: \"Error response from daemon: pull access denied for fastapi-k8s-demo, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/fastapi-pod" podUID="71c7a2ee-b30f-4f8a-8991-3305284c8ffb"
Nov 18 15:25:04 minikube kubelet[2317]: E1118 15:25:04.843773    2317 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fastapi\" with ImagePullBackOff: \"Back-off pulling image \\\"fastapi-k8s-demo:latest\\\": ErrImagePull: Error response from daemon: pull access denied for fastapi-k8s-demo, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/fastapi-pod" podUID="71c7a2ee-b30f-4f8a-8991-3305284c8ffb"
Nov 18 15:25:20 minikube kubelet[2317]: E1118 15:25:20.256740    2317 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for fastapi-k8s-demo, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="fastapi-k8s-demo:latest"
Nov 18 15:25:20 minikube kubelet[2317]: E1118 15:25:20.256790    2317 kuberuntime_image.go:43] "Failed to pull image" err="Error response from daemon: pull access denied for fastapi-k8s-demo, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="fastapi-k8s-demo:latest"
Nov 18 15:25:20 minikube kubelet[2317]: E1118 15:25:20.256865    2317 kuberuntime_manager.go:1449] "Unhandled Error" err="container fastapi start failed in pod fastapi-pod_default(71c7a2ee-b30f-4f8a-8991-3305284c8ffb): ErrImagePull: Error response from daemon: pull access denied for fastapi-k8s-demo, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
Nov 18 15:25:20 minikube kubelet[2317]: E1118 15:25:20.256894    2317 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fastapi\" with ErrImagePull: \"Error response from daemon: pull access denied for fastapi-k8s-demo, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/fastapi-pod" podUID="71c7a2ee-b30f-4f8a-8991-3305284c8ffb"
Nov 18 15:25:34 minikube kubelet[2317]: E1118 15:25:34.907142    2317 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fastapi\" with ImagePullBackOff: \"Back-off pulling image \\\"fastapi-k8s-demo:latest\\\": ErrImagePull: Error response from daemon: pull access denied for fastapi-k8s-demo, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/fastapi-pod" podUID="71c7a2ee-b30f-4f8a-8991-3305284c8ffb"
Nov 18 15:25:47 minikube kubelet[2317]: E1118 15:25:47.225283    2317 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for fastapi-k8s-demo, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="fastapi-k8s-demo:latest"
Nov 18 15:25:47 minikube kubelet[2317]: E1118 15:25:47.225350    2317 kuberuntime_image.go:43] "Failed to pull image" err="Error response from daemon: pull access denied for fastapi-k8s-demo, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="fastapi-k8s-demo:latest"
Nov 18 15:25:47 minikube kubelet[2317]: E1118 15:25:47.225435    2317 kuberuntime_manager.go:1449] "Unhandled Error" err="container fastapi start failed in pod fastapi-pod_default(71c7a2ee-b30f-4f8a-8991-3305284c8ffb): ErrImagePull: Error response from daemon: pull access denied for fastapi-k8s-demo, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
Nov 18 15:25:47 minikube kubelet[2317]: E1118 15:25:47.225463    2317 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fastapi\" with ErrImagePull: \"Error response from daemon: pull access denied for fastapi-k8s-demo, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/fastapi-pod" podUID="71c7a2ee-b30f-4f8a-8991-3305284c8ffb"
Nov 18 15:25:58 minikube kubelet[2317]: E1118 15:25:58.906049    2317 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fastapi\" with ImagePullBackOff: \"Back-off pulling image \\\"fastapi-k8s-demo:latest\\\": ErrImagePull: Error response from daemon: pull access denied for fastapi-k8s-demo, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/fastapi-pod" podUID="71c7a2ee-b30f-4f8a-8991-3305284c8ffb"
Nov 18 15:26:10 minikube kubelet[2317]: E1118 15:26:10.907003    2317 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fastapi\" with ImagePullBackOff: \"Back-off pulling image \\\"fastapi-k8s-demo:latest\\\": ErrImagePull: Error response from daemon: pull access denied for fastapi-k8s-demo, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/fastapi-pod" podUID="71c7a2ee-b30f-4f8a-8991-3305284c8ffb"
Nov 18 15:26:24 minikube kubelet[2317]: E1118 15:26:24.907256    2317 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fastapi\" with ImagePullBackOff: \"Back-off pulling image \\\"fastapi-k8s-demo:latest\\\": ErrImagePull: Error response from daemon: pull access denied for fastapi-k8s-demo, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/fastapi-pod" podUID="71c7a2ee-b30f-4f8a-8991-3305284c8ffb"
Nov 18 15:26:39 minikube kubelet[2317]: E1118 15:26:39.245591    2317 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for fastapi-k8s-demo, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="fastapi-k8s-demo:latest"
Nov 18 15:26:39 minikube kubelet[2317]: E1118 15:26:39.245637    2317 kuberuntime_image.go:43] "Failed to pull image" err="Error response from daemon: pull access denied for fastapi-k8s-demo, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="fastapi-k8s-demo:latest"
Nov 18 15:26:39 minikube kubelet[2317]: E1118 15:26:39.245710    2317 kuberuntime_manager.go:1449] "Unhandled Error" err="container fastapi start failed in pod fastapi-pod_default(71c7a2ee-b30f-4f8a-8991-3305284c8ffb): ErrImagePull: Error response from daemon: pull access denied for fastapi-k8s-demo, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
Nov 18 15:26:39 minikube kubelet[2317]: E1118 15:26:39.245743    2317 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fastapi\" with ErrImagePull: \"Error response from daemon: pull access denied for fastapi-k8s-demo, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/fastapi-pod" podUID="71c7a2ee-b30f-4f8a-8991-3305284c8ffb"
Nov 18 15:26:43 minikube kubelet[2317]: I1118 15:26:43.936514    2317 reconciler_common.go:163] "operationExecutor.UnmountVolume started for volume \"kube-api-access-bff9l\" (UniqueName: \"kubernetes.io/projected/71c7a2ee-b30f-4f8a-8991-3305284c8ffb-kube-api-access-bff9l\") pod \"71c7a2ee-b30f-4f8a-8991-3305284c8ffb\" (UID: \"71c7a2ee-b30f-4f8a-8991-3305284c8ffb\") "
Nov 18 15:26:43 minikube kubelet[2317]: I1118 15:26:43.938505    2317 operation_generator.go:781] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/71c7a2ee-b30f-4f8a-8991-3305284c8ffb-kube-api-access-bff9l" (OuterVolumeSpecName: "kube-api-access-bff9l") pod "71c7a2ee-b30f-4f8a-8991-3305284c8ffb" (UID: "71c7a2ee-b30f-4f8a-8991-3305284c8ffb"). InnerVolumeSpecName "kube-api-access-bff9l". PluginName "kubernetes.io/projected", VolumeGIDValue ""
Nov 18 15:26:44 minikube kubelet[2317]: I1118 15:26:44.037392    2317 reconciler_common.go:299] "Volume detached for volume \"kube-api-access-bff9l\" (UniqueName: \"kubernetes.io/projected/71c7a2ee-b30f-4f8a-8991-3305284c8ffb-kube-api-access-bff9l\") on node \"minikube\" DevicePath \"\""
Nov 18 15:26:44 minikube kubelet[2317]: I1118 15:26:44.911538    2317 kubelet_volumes.go:163] "Cleaned up orphaned pod volumes dir" podUID="71c7a2ee-b30f-4f8a-8991-3305284c8ffb" path="/var/lib/kubelet/pods/71c7a2ee-b30f-4f8a-8991-3305284c8ffb/volumes"
Nov 18 15:26:46 minikube kubelet[2317]: I1118 15:26:46.152873    2317 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-dk4z8\" (UniqueName: \"kubernetes.io/projected/ed655f9e-4ee1-4447-8afd-9d2a8d026638-kube-api-access-dk4z8\") pod \"fastapi-pod\" (UID: \"ed655f9e-4ee1-4447-8afd-9d2a8d026638\") " pod="default/fastapi-pod"
Nov 18 15:26:47 minikube kubelet[2317]: I1118 15:26:47.284288    2317 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="default/fastapi-pod" podStartSLOduration=1.284272319 podStartE2EDuration="1.284272319s" podCreationTimestamp="2025-11-18 15:26:46 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-11-18 15:26:47.28419757 +0000 UTC m=+1528.456882118" watchObservedRunningTime="2025-11-18 15:26:47.284272319 +0000 UTC m=+1528.456956866"


==> storage-provisioner [8ca16d1792df] <==
W1118 15:31:35.864304       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1118 15:31:35.868724       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1118 15:31:37.871217       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1118 15:31:37.875689       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1118 15:31:39.879109       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1118 15:31:39.883817       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1118 15:31:41.887281       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1118 15:31:41.890606       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1118 15:31:43.893040       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1118 15:31:43.898024       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1118 15:31:45.900197       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1118 15:31:45.905027       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1118 15:31:47.908166       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1118 15:31:47.911268       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1118 15:31:49.913614       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1118 15:31:49.921123       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1118 15:31:51.924471       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1118 15:31:51.927798       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1118 15:31:53.930204       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1118 15:31:53.933685       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1118 15:31:55.936619       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1118 15:31:55.940165       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1118 15:31:57.943421       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1118 15:31:57.946996       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1118 15:31:59.949470       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1118 15:31:59.954173       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1118 15:32:01.957396       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1118 15:32:01.963224       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1118 15:32:03.965618       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1118 15:32:03.969123       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1118 15:32:05.972632       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1118 15:32:05.976429       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1118 15:32:07.979451       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1118 15:32:07.983381       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1118 15:32:09.985553       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1118 15:32:09.989982       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1118 15:32:11.993720       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1118 15:32:11.997318       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1118 15:32:14.000397       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1118 15:32:14.004183       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1118 15:32:16.007268       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1118 15:32:16.011986       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1118 15:32:18.014658       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1118 15:32:18.018156       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1118 15:32:20.020460       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1118 15:32:20.023852       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1118 15:32:22.026550       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1118 15:32:22.030846       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1118 15:32:24.033943       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1118 15:32:24.037403       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1118 15:32:26.040385       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1118 15:32:26.044039       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1118 15:32:28.047149       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1118 15:32:28.050594       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1118 15:32:30.053843       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1118 15:32:30.057609       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1118 15:32:32.060692       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1118 15:32:32.064245       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1118 15:32:34.067194       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1118 15:32:34.070680       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice

