{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d699e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url = \"https://integrate.api.nvidia.com/v1\",\n",
    "    api_key = \"nvapi-\"\n",
    ")\n",
    "\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"speakleash/bielik-11b-v2.6-instruct\", \n",
    "    messages=[{\"role\":\"user\",\n",
    "               \"content\":\"Znasz LangChain?\"}],\n",
    "    temperature=0.2,\n",
    "    top_p=0.7,\n",
    "    max_tokens=1024,\n",
    "    stream=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ccde59c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tak, LangChain to biblioteka open-source stworzona przez SpeakLeash, która umożliwia łatwe tworzenie zaawansowanych modeli językowych, takich jak GPT-3, GPT-2, BERT, T5 i innych. Jest napisana w Pythonie i wykorzystuje biblioteki takie jak Hugging Face Transformers oraz PyTorch lub TensorFlow.\n",
      "\n",
      "LangChain oferuje kilka kluczowych funkcji:\n",
      "\n",
      "1. **Łatwe wdrażanie modeli**: Umożliwia szybkie wdrożenie różnych modeli językowych bez konieczności głębokiej wiedzy na temat ich architektury.\n",
      "2. **Generowanie tekstu**: Pozwala na generowanie tekstu na podstawie zadanych promptów.\n",
      "3. **Funkcje językowe**: Oferuje różne funkcje, takie jak tłumaczenie, podsumowywanie, odpowiadanie na pytania itp.\n",
      "4. **Integracja z modelami**: Można łatwo zintegrować LangChain z istniejącymi modelami, w tym z własnymi modelami.\n",
      "\n",
      "LangChain jest przydatny dla deweloperów, którzy chcą szybko zacząć pracę z modelami językowymi bez konieczności zrozumienia wszystkich szczegółów technicznych. Jest to narzędzie, które może znacznie przyspieszyć proces tworzenia aplikacji wykorzystujących zaawansowane możliwości językowe.\n",
      "\n",
      "Jeśli masz konkretne pytania dotyczące LangChain, chętnie na nie odpowiem!"
     ]
    }
   ],
   "source": [
    "\n",
    "for chunk in completion:\n",
    " if chunk.choices[0].delta.content is not None:\n",
    "   print(chunk.choices[0].delta.content, end=\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48db4822",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# --- 1. KLIENT DO SAFETY GUARDA (NVIDIA) ---\n",
    "guard_client = OpenAI(\n",
    "  base_url = \"https://integrate.api.nvidia.com/v1\",\n",
    "  api_key = \"nvapi-\"\n",
    ")\n",
    "\n",
    "\n",
    "# --- 2. KLIENT DO BIELIKA ---\n",
    "\n",
    "bielik_client = OpenAI(\n",
    "  base_url = \"https://integrate.api.nvidia.com/v1\",\n",
    "  api_key = \"nvapi-\"\n",
    ")\n",
    "\n",
    "user_input = \"Co to jest LangChain?\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ea110f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SafetyGuard: {\"User Safety\": \"safe\", \"Response Safety\": \"safe\"} \n"
     ]
    }
   ],
   "source": [
    "# --- 3. Zapytanie do modelu Safety Guard ---\n",
    "guard_response = guard_client.chat.completions.create(\n",
    "   model=\"nvidia/llama-3.1-nemotron-safety-guard-8b-v3\",\n",
    "   messages=[\n",
    "       {\"role\": \"user\", \"content\": user_input},\n",
    "       {\"role\": \"assistant\", \"content\": \"Obawiam się, że nie mogę pomóc w tej sprawie.\"}\n",
    "   ],\n",
    "   stream=False\n",
    ")\n",
    "guard_message = guard_response.choices[0].message.content\n",
    "print(\"SafetyGuard:\", guard_message)\n",
    "\n",
    "is_safe = \"unsafe\" not in guard_message.lower() \n",
    " # prosty przykład\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "61008a80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain to platforma open-source, która umożliwia tworzenie i wdrażanie zaawansowanych modeli językowych, w tym dużych modeli językowych (LLM - Large Language Models). Jest to zestaw narzędzi i bibliotek, które ułatwiają pracę z modelami takimi jak GPT-3, GPT-4, czy inne modele oparte na architekturze transformerowej.\n",
      "\n",
      "Główne cechy LangChain to:\n",
      "\n",
      "1. **Wsparcie dla różnych modeli**: LangChain obsługuje wiele różnych modeli językowych, w tym te dostępne w ramach projektów open-science, takich jak Speakleash (dawniej OpenAI API), EleutherAI, czy inne.\n",
      "\n",
      "2. **Interfejs API**: Umożliwia łatwe wywoływanie modeli językowych poprzez interfejs API, co pozwala na integrację z różnymi aplikacjami i systemami.\n",
      "\n",
      "3. **Optymalizacja wydajności**: LangChain oferuje narzędzia do optymalizacji wydajności modeli, takie jak tuning parametrów, zarządzanie zasobami (np. pamięcią), oraz techniki redukcji kosztów obliczeniowych.\n",
      "\n",
      "4. **Wsparcie dla różnych języków**: Platforma obsługuje wiele języków, co pozwala na tworzenie modeli wielojęzycznych i ich wykorzystanie w różnych kontekstach.\n",
      "\n",
      "5. **Edukacja i wsparcie społeczności**: LangChain ma aktywną społeczność, która oferuje wsparcie i zasoby edukacyjne dla użytkowników chcących nauczyć się korzystać z modeli językowych.\n",
      "\n",
      "6. **Etyczne podejście**: Projekt kładzie nacisk na etyczne wykorzystanie modeli językowych, promując odpowiedzialne praktyki w zakresie ich użycia.\n",
      "\n",
      "LangChain jest używany zarówno przez badaczy, jak i przez deweloperów, którzy chcą tworzyć innowacyjne aplikacje wykorzystujące zaawansowane modele językowe. Dzięki swojej elastyczności i otwartości, LangChain staje się popularnym narzędziem w dziedzinie przetwarzania języka naturalnego (NLP)."
     ]
    }
   ],
   "source": [
    "if not is_safe:\n",
    "   print(\"Bielik: Obawiam się, że nie mogę pomóc w tej sprawie.\")\n",
    "else:\n",
    "   # --- 4. Dopiero teraz odpytujemy Bielika ---\n",
    "   stream = bielik_client.chat.completions.create(\n",
    "       model=\"speakleash/bielik-11b-v2.6-instruct\",\n",
    "       messages=[\n",
    "           {\"role\": \"system\", \"content\": \"Odpowiadaj pomocnie i jasno.\"},\n",
    "           {\"role\": \"user\", \"content\": user_input}\n",
    "       ],\n",
    "       temperature=0.2,\n",
    "       top_p=0.7,\n",
    "       max_tokens=1024,\n",
    "       stream=True\n",
    "   )\n",
    "   for chunk in stream:\n",
    "        if chunk.choices and chunk.choices[0].delta:\n",
    "            token = chunk.choices[0].delta.content or \"\"\n",
    "            print(token, end=\"\", flush=True)\n",
    "\n",
    "   #print(\"Bielik:\", completion.choices[0].message.content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b705d861",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
